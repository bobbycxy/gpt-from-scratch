{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualising dynamic padding with causal self attention\n",
    "\n",
    "Typically, the causal self attention behaves as having the upper part of the triangle, disregarding the diagonal line, as being cast as float('-inf'). This ensures that no token at an instance of time can look to the future tokens. However, this assumes that the inputs are all filled to the max block size (T) of the model. \n",
    "\n",
    "What happens if the inputs are not in a similar length, and we have to apply padding to them just so that we can feed them in a batch? \n",
    "\n",
    "This document attempts to visualise what happens in the hood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create a fake scenario of the attention. Goal\n",
    "## here is to incorporate padding and mask the padding\n",
    "\n",
    "## first, create the input matrix\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.functional import F \n",
    "\n",
    "input_ids = torch.tensor([[1, 2, 3, 0, 0], # padding_token_id = 0\n",
    "                  [6, 7, 8, 9, 10], \n",
    "                  [11, 12, 0, 0, 0]]) # batch_size x seq_len\n",
    "\n",
    "B = input_ids.size(0) # 3\n",
    "T = input_ids.size(1) # 5\n",
    "C = 4 # n_embed\n",
    "h_s = 3 # head_size\n",
    "dropout = 0.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 5, 4])\n",
      "tensor([[[-0.5822, -0.7915,  1.6073,  0.5012],\n",
      "         [-1.2199,  1.1544, -1.4182, -0.3525],\n",
      "         [-1.4957, -0.2008, -0.7390,  0.3103],\n",
      "         [ 1.0110, -1.1545, -0.6795, -1.0732],\n",
      "         [ 1.0110, -1.1545, -0.6795, -1.0732]],\n",
      "\n",
      "        [[ 0.4927, -1.0773, -0.6775, -1.2340],\n",
      "         [ 0.2375, -0.0274, -0.2853, -0.8249],\n",
      "         [ 0.0331, -0.8663,  0.3287,  1.3669],\n",
      "         [ 0.7534,  0.2125,  1.2789,  0.6051],\n",
      "         [-1.7066,  0.9056, -0.2907, -0.1510]],\n",
      "\n",
      "        [[-1.3441,  0.6079, -0.7126, -0.5544],\n",
      "         [-0.3601, -0.1667, -0.5483, -1.1857],\n",
      "         [ 1.0110, -1.1545, -0.6795, -1.0732],\n",
      "         [ 1.0110, -1.1545, -0.6795, -1.0732],\n",
      "         [ 1.0110, -1.1545, -0.6795, -1.0732]]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "position_embedding = nn.Embedding(100, C)\n",
    "\n",
    "x = position_embedding(input_ids)\n",
    "\n",
    "print(x.size())\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating the scenario of inside the causal self attention\n",
    "\n",
    "## create the weights of q, k, v\n",
    "q = nn.Linear(C, h_s)\n",
    "k = nn.Linear(C, h_s)\n",
    "v = nn.Linear(C, h_s)\n",
    "\n",
    "## we will not create the attention mask here. That will be created layer\n",
    "\n",
    "## create dropout\n",
    "dropout = nn.Dropout(dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0.],\n",
       "        [1., 1., 1., 0., 0.],\n",
       "        [1., 1., 1., 1., 0.],\n",
       "        [1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Normally, this is how the attention mask will look\n",
    "torch.tril(torch.ones(T, T)) # used to mask the attention matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## perform matrix multiplication of the weights with the input\n",
    "Q = q(x)\n",
    "K = k(x)\n",
    "V = v(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 5, 5])\n",
      "tensor([[[-0.7093,  0.2768, -0.0566,  0.3933,  0.3933],\n",
      "         [ 0.1542, -1.2993, -0.8118, -0.0039, -0.0039],\n",
      "         [-0.1514, -1.3935, -0.9908,  0.1829,  0.1829],\n",
      "         [ 0.2091, -0.9234, -0.4266, -0.0906, -0.0906],\n",
      "         [ 0.2091, -0.9234, -0.4266, -0.0906, -0.0906]],\n",
      "\n",
      "        [[-0.2317, -0.4375,  0.3497,  0.1786, -1.0139],\n",
      "         [-0.0871, -0.2021,  0.1585,  0.0427, -0.4850],\n",
      "         [ 0.1949,  0.0660, -0.2792, -0.2333, -0.2043],\n",
      "         [ 0.3182,  0.3802, -0.3170, -0.3909,  0.7895],\n",
      "         [ 0.0429, -0.2189, -0.1106, -0.0816, -0.9293]],\n",
      "\n",
      "        [[-1.0239, -0.5582,  0.1024,  0.1024,  0.1024],\n",
      "         [-0.9580, -0.5921,  0.0225,  0.0225,  0.0225],\n",
      "         [-0.7755, -0.5549, -0.0906, -0.0906, -0.0906],\n",
      "         [-0.7755, -0.5549, -0.0906, -0.0906, -0.0906],\n",
      "         [-0.7755, -0.5549, -0.0906, -0.0906, -0.0906]]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "## Calculate the numerator of the attention\n",
    "attention = Q @ K.transpose(-2,-1) * (C ** -0.5) # B,T,T\n",
    "print(attention.shape)\n",
    "print(attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 5, 5])\n",
      "tensor([[[False,  True,  True,  True,  True],\n",
      "         [False, False,  True,  True,  True],\n",
      "         [False, False, False,  True,  True],\n",
      "         [ True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True]],\n",
      "\n",
      "        [[False,  True,  True,  True,  True],\n",
      "         [False, False,  True,  True,  True],\n",
      "         [False, False, False,  True,  True],\n",
      "         [False, False, False, False,  True],\n",
      "         [False, False, False, False, False]],\n",
      "\n",
      "        [[False,  True,  True,  True,  True],\n",
      "         [False, False,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True]]])\n"
     ]
    }
   ],
   "source": [
    "## here, create the causal mask and the padding mask. The causal mask is similar \n",
    "## to the one we usually create. The padding mask is created by checking if the input has\n",
    "## padding tokens. If it has padding tokens, then the mask is 1, else 0.\n",
    "\n",
    "# Create causal mask tensor (batch_size, seq_len, seq_len)\n",
    "causal_mask = torch.triu(torch.ones(T, T), diagonal=1).unsqueeze(0)  # Upper triangular matrix\n",
    "causal_mask = causal_mask == 1 ## converts into BoolTensor\n",
    "\n",
    "# Create padding mask tensor (batch_size, seq_len)\n",
    "padding_mask = (input_ids == 0).unsqueeze(1)  # Padding tokens are zeros\n",
    "\n",
    "# Combine the masks (padding_mask will have zeros for padded tokens)\n",
    "mask = causal_mask + padding_mask.unsqueeze(-1) ## True + False = True\n",
    "mask = mask.squeeze(1)\n",
    "print(mask.shape)\n",
    "print(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 5, 5])\n",
      "tensor([[[-0.7093,    -inf,    -inf,    -inf,    -inf],\n",
      "         [ 0.1542, -1.2993,    -inf,    -inf,    -inf],\n",
      "         [-0.1514, -1.3935, -0.9908,    -inf,    -inf],\n",
      "         [   -inf,    -inf,    -inf,    -inf,    -inf],\n",
      "         [   -inf,    -inf,    -inf,    -inf,    -inf]],\n",
      "\n",
      "        [[-0.2317,    -inf,    -inf,    -inf,    -inf],\n",
      "         [-0.0871, -0.2021,    -inf,    -inf,    -inf],\n",
      "         [ 0.1949,  0.0660, -0.2792,    -inf,    -inf],\n",
      "         [ 0.3182,  0.3802, -0.3170, -0.3909,    -inf],\n",
      "         [ 0.0429, -0.2189, -0.1106, -0.0816, -0.9293]],\n",
      "\n",
      "        [[-1.0239,    -inf,    -inf,    -inf,    -inf],\n",
      "         [-0.9580, -0.5921,    -inf,    -inf,    -inf],\n",
      "         [   -inf,    -inf,    -inf,    -inf,    -inf],\n",
      "         [   -inf,    -inf,    -inf,    -inf,    -inf],\n",
      "         [   -inf,    -inf,    -inf,    -inf,    -inf]]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n"
     ]
    }
   ],
   "source": [
    "## masked fill the attention matrix with the mask\n",
    "attention = attention.masked_fill(mask, float('-inf')) # B,T,T\n",
    "print(attention.size())\n",
    "print(attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 5, 5])\n",
      "tensor([[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.8105, 0.1895, 0.0000, 0.0000, 0.0000],\n",
      "         [0.5811, 0.1678, 0.2510, 0.0000, 0.0000],\n",
      "         [   nan,    nan,    nan,    nan,    nan],\n",
      "         [   nan,    nan,    nan,    nan,    nan]],\n",
      "\n",
      "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.5287, 0.4713, 0.0000, 0.0000, 0.0000],\n",
      "         [0.3998, 0.3514, 0.2488, 0.0000, 0.0000],\n",
      "         [0.3241, 0.3448, 0.1717, 0.1595, 0.0000],\n",
      "         [0.2572, 0.1979, 0.2206, 0.2271, 0.0973]],\n",
      "\n",
      "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.4095, 0.5905, 0.0000, 0.0000, 0.0000],\n",
      "         [   nan,    nan,    nan,    nan,    nan],\n",
      "         [   nan,    nan,    nan,    nan,    nan],\n",
      "         [   nan,    nan,    nan,    nan,    nan]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "## now, apply the softmax to the attention matrix\n",
    "attention = F.softmax(attention, dim=-1)\n",
    "print(attention.size())\n",
    "print(attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 5, 5])\n",
      "tensor([[[1.1111, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.9006, 0.2105, 0.0000, 0.0000, 0.0000],\n",
      "         [0.6457, 0.1865, 0.2789, 0.0000, 0.0000],\n",
      "         [   nan,    nan,    nan,    nan,    nan],\n",
      "         [   nan,    nan,    nan,    nan,    nan]],\n",
      "\n",
      "        [[1.1111, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.5875, 0.5236, 0.0000, 0.0000, 0.0000],\n",
      "         [0.4442, 0.3904, 0.0000, 0.0000, 0.0000],\n",
      "         [0.3601, 0.3831, 0.1908, 0.1772, 0.0000],\n",
      "         [0.2857, 0.2199, 0.2451, 0.2523, 0.1081]],\n",
      "\n",
      "        [[1.1111, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.4551, 0.6561, 0.0000, 0.0000, 0.0000],\n",
      "         [   nan,    nan,    nan,    nan,    nan],\n",
      "         [   nan,    nan,    nan,    nan,    nan],\n",
      "         [   nan,    nan,    nan,    nan,    nan]]], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "## apply dropout\n",
    "attention = dropout(attention)\n",
    "print(attention.size())\n",
    "print(attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 5, 3])\n",
      "tensor([[[ 0.5873, -0.6878, -0.1615],\n",
      "         [ 0.3643, -0.3133, -0.1221],\n",
      "         [ 0.1727, -0.0221, -0.1713],\n",
      "         [    nan,     nan,     nan],\n",
      "         [    nan,     nan,     nan]],\n",
      "\n",
      "        [[ 0.2732, -0.1841,  0.2674],\n",
      "         [ 0.2040, -0.0867,  0.3217],\n",
      "         [ 0.1536, -0.0657,  0.2413],\n",
      "         [ 0.1762, -0.4011,  0.4033],\n",
      "         [ 0.1372, -0.4055,  0.3681]],\n",
      "\n",
      "        [[-0.2119,  1.0908, -0.2169],\n",
      "         [-0.0069,  0.7432, -0.0770],\n",
      "         [    nan,     nan,     nan],\n",
      "         [    nan,     nan,     nan],\n",
      "         [    nan,     nan,     nan]]], grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "## lastly, multiply the attention with the value matrix\n",
    "out = attention @ V\n",
    "print(out.size())\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear = nn.Linear(h_s, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.5453,  0.6221,  0.1022, -0.2521],\n",
       "         [-0.4833,  0.5375,  0.2063, -0.3706],\n",
       "         [-0.4284,  0.4553,  0.3124, -0.4826],\n",
       "         [    nan,     nan,     nan,     nan],\n",
       "         [    nan,     nan,     nan,     nan]],\n",
       "\n",
       "        [[-0.5044,  0.6096,  0.1712, -0.3612],\n",
       "         [-0.4943,  0.6017,  0.1944, -0.3903],\n",
       "         [-0.4859,  0.5841,  0.2351, -0.4258],\n",
       "         [-0.5817,  0.7515,  0.1841, -0.3674],\n",
       "         [-0.5831,  0.7530,  0.2096, -0.3893]],\n",
       "\n",
       "        [[-0.1971,  0.0984,  0.5250, -0.7377],\n",
       "         [-0.2740,  0.2232,  0.3928, -0.6022],\n",
       "         [    nan,     nan,     nan,     nan],\n",
       "         [    nan,     nan,     nan,     nan],\n",
       "         [    nan,     nan,     nan,     nan]]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.5453,  0.6221,  0.1022, -0.2521],\n",
       "         [-0.4833,  0.5375,  0.2063, -0.3706],\n",
       "         [-0.4284,  0.4553,  0.3124, -0.4826],\n",
       "         [    nan,     nan,     nan,     nan],\n",
       "         [    nan,     nan,     nan,     nan]],\n",
       "\n",
       "        [[-0.5044,  0.6096,  0.1712, -0.3612],\n",
       "         [-0.4943,  0.6017,  0.1944, -0.3903],\n",
       "         [-0.4859,  0.5841,  0.2351, -0.4258],\n",
       "         [-0.5817,  0.7515,  0.1841, -0.3674],\n",
       "         [-0.5831,  0.7530,  0.2096, -0.3893]],\n",
       "\n",
       "        [[-0.1971,  0.0984,  0.5250, -0.7377],\n",
       "         [-0.2740,  0.2232,  0.3928, -0.6022],\n",
       "         [    nan,     nan,     nan,     nan],\n",
       "         [    nan,     nan,     nan,     nan],\n",
       "         [    nan,     nan,     nan,     nan]]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 5, 4])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear(out).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0433,  0.1076,  0.2371],\n",
       "         [ 0.1520, -0.3164,  0.3542],\n",
       "         [ 0.2311, -0.4924,  0.4840],\n",
       "         [ 0.1870, -0.2289,  0.1277],\n",
       "         [ 0.1870, -0.2289,  0.1277]],\n",
       "\n",
       "        [[ 0.0853, -0.6197,  0.4003],\n",
       "         [ 0.0524, -0.5504,  0.3384],\n",
       "         [ 0.0316,  0.0387,  0.1955],\n",
       "         [-0.1959,  1.0176, -0.2850],\n",
       "         [    nan,     nan,     nan]],\n",
       "\n",
       "        [[ 0.2164, -0.3653,  0.4213],\n",
       "         [ 0.2311, -0.4924,  0.4840],\n",
       "         [ 0.1679, -0.1901,  0.3402],\n",
       "         [ 0.1679, -0.1901,  0.3402],\n",
       "         [ 0.1679, -0.1901,  0.3402]]], grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.scaled_dot_product_attention(Q, K, V, mask, is_causal=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_bias = torch.zeros(Q.size(-2), K.size(-2), dtype = Q.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_bias.unsqueeze(0).expand(3, -1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[False,  True,  True,  True,  True],\n",
       "         [False, False,  True,  True,  True],\n",
       "         [False, False, False,  True,  True],\n",
       "         [ True,  True,  True,  True,  True],\n",
       "         [ True,  True,  True,  True,  True]],\n",
       "\n",
       "        [[False,  True,  True,  True,  True],\n",
       "         [False, False,  True,  True,  True],\n",
       "         [False, False, False,  True,  True],\n",
       "         [False, False, False, False,  True],\n",
       "         [False, False, False, False, False]],\n",
       "\n",
       "        [[False,  True,  True,  True,  True],\n",
       "         [False, False,  True,  True,  True],\n",
       "         [ True,  True,  True,  True,  True],\n",
       "         [ True,  True,  True,  True,  True],\n",
       "         [ True,  True,  True,  True,  True]]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "~mask.logical_not()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_968052/41538528.py:1: UserWarning: Use of masked_fill_ on expanded tensors is deprecated. Please clone() the tensor before performing this operation. This also applies to advanced indexing e.g. tensor[mask] = scalar (Triggered internally at /opt/conda/conda-bld/pytorch_1712608853099/work/aten/src/ATen/native/TensorAdvancedIndexing.cpp:1914.)\n",
      "  attn_bias.unsqueeze(0).expand(3, -1, -1).masked_fill_(~mask.logical_not(), float('-inf'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-inf, -inf, -inf, -inf, -inf],\n",
       "         [-inf, -inf, -inf, -inf, -inf],\n",
       "         [-inf, -inf, -inf, -inf, -inf],\n",
       "         [-inf, -inf, -inf, -inf, -inf],\n",
       "         [-inf, -inf, -inf, -inf, -inf]],\n",
       "\n",
       "        [[-inf, -inf, -inf, -inf, -inf],\n",
       "         [-inf, -inf, -inf, -inf, -inf],\n",
       "         [-inf, -inf, -inf, -inf, -inf],\n",
       "         [-inf, -inf, -inf, -inf, -inf],\n",
       "         [-inf, -inf, -inf, -inf, -inf]],\n",
       "\n",
       "        [[-inf, -inf, -inf, -inf, -inf],\n",
       "         [-inf, -inf, -inf, -inf, -inf],\n",
       "         [-inf, -inf, -inf, -inf, -inf],\n",
       "         [-inf, -inf, -inf, -inf, -inf],\n",
       "         [-inf, -inf, -inf, -inf, -inf]]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_bias.unsqueeze(0).expand(3, -1, -1).masked_fill_(~mask.logical_not(), float('-inf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def scaled_dot_product_attention(query, key, value, attn_mask=None, dropout_p=0.0, is_causal=False, scale=None) -> torch.Tensor:\n",
    "    L, S = query.size(-2), key.size(-2)\n",
    "    scale_factor = 1 / math.sqrt(query.size(-1)) if scale is None else scale\n",
    "    attn_bias = torch.zeros(L, S, dtype=query.dtype)\n",
    "    if is_causal:\n",
    "        assert attn_mask is None\n",
    "        temp_mask = torch.ones(L, S, dtype=torch.bool).tril(diagonal=0)\n",
    "        attn_bias.masked_fill_(temp_mask.logical_not(), float(\"-inf\"))\n",
    "        attn_bias.to(query.dtype)\n",
    "\n",
    "    if attn_mask is not None:\n",
    "        if attn_mask.dtype == torch.bool:\n",
    "            attn_bias.masked_fill_(attn_mask.logical_not(), float(\"-inf\"))\n",
    "        else:\n",
    "            attn_bias += attn_mask\n",
    "    attn_weight = query @ key.transpose(-2, -1) * scale_factor\n",
    "    attn_weight += attn_bias\n",
    "    attn_weight = torch.softmax(attn_weight, dim=-1)\n",
    "    attn_weight = torch.dropout(attn_weight, dropout_p, train=True)\n",
    "    return attn_weight @ value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "output with shape [5, 5] doesn't match the broadcast shape [3, 5, 5]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m scaled_dot_product_attention(Q, K, V, mask, is_causal\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[31], line 15\u001b[0m, in \u001b[0;36mscaled_dot_product_attention\u001b[0;34m(query, key, value, attn_mask, dropout_p, is_causal, scale)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attn_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attn_mask\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mbool:\n\u001b[0;32m---> 15\u001b[0m         attn_bias\u001b[38;5;241m.\u001b[39mmasked_fill_(attn_mask\u001b[38;5;241m.\u001b[39mlogical_not(), \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-inf\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m         attn_bias \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m attn_mask\n",
      "\u001b[0;31mRuntimeError\u001b[0m: output with shape [5, 5] doesn't match the broadcast shape [3, 5, 5]"
     ]
    }
   ],
   "source": [
    "scaled_dot_product_attention(Q, K, V, mask, is_causal=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0410, -1.0452, -0.2190],\n",
       "         [ 0.0987, -1.1155, -0.0409],\n",
       "         [ 0.2031, -1.0693,  0.1037],\n",
       "         [-0.2137, -0.8735, -0.5728],\n",
       "         [-0.2137, -0.8735, -0.5728]],\n",
       "\n",
       "        [[-0.3442,  0.0989, -0.0902],\n",
       "         [-0.0428, -0.2093,  0.2775],\n",
       "         [-0.0196, -0.2133,  0.3251],\n",
       "         [ 0.1189, -0.5733,  0.3315],\n",
       "         [    nan,     nan,     nan]],\n",
       "\n",
       "        [[ 0.0716, -0.9821, -0.0615],\n",
       "         [ 0.2031, -1.0693,  0.1037],\n",
       "         [-0.3784, -0.5380, -0.5431],\n",
       "         [-0.3784, -0.5380, -0.5431],\n",
       "         [-0.3784, -0.5380, -0.5431]]], grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.scaled_dot_product_attention(Q, K, V, mask, is_causal=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "smallville",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
