{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "input_tokens = torch.tensor([\n",
    "    [0, 1, 2, 3, 4, 5, 6, 7, 4999, 4999],\n",
    "    [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
    "])\n",
    "\n",
    "def generate_random_token_ids(shape, vocab_size, pad_token_id, mask_token_id):\n",
    "    random_token_ids = torch.randint(0, vocab_size, shape)\n",
    "    # Ensure random_token_ids do not contain pad_token_id or mask_token_id\n",
    "    invalid_ids = {pad_token_id, mask_token_id}\n",
    "    for i in range(random_token_ids.numel()):\n",
    "        while random_token_ids.view(-1)[i].item() in invalid_ids:\n",
    "            random_token_ids.view(-1)[i] = torch.randint(0, vocab_size, (1,))\n",
    "    return random_token_ids\n",
    "\n",
    "def masked_lm(input_ids, mlm_prob = 0.15, pad_token_id = 4999, mask_token_id = 5000, vocab_size = 10000):\n",
    "    '''\n",
    "    Masked language model\n",
    "    '''\n",
    "    mask = torch.rand(input_ids.size()) < mlm_prob # create a mask of true and false\n",
    "    mask &= input_ids != pad_token_id # do not mask padding\n",
    "    mask &= input_ids != mask_token_id # do not mask mask token\n",
    "\n",
    "    ## create clones\n",
    "    mlm_data = input_ids.clone()\n",
    "    labels = input_ids.clone()\n",
    "\n",
    "    ## get the indices of the mask\n",
    "    mask_idx = mask.nonzero(as_tuple=True)\n",
    "\n",
    "    ## randomly mask tokens\n",
    "    mask_idx_shuffle = torch.randperm(mask_idx[0].shape[0])\n",
    "\n",
    "    ## get the tokens for each type of masking (mask, random, keep)\n",
    "    tomask_idx = mask_idx_shuffle[:int(mask_idx[0].shape[0] * 0.8)]\n",
    "    torandom_idx = mask_idx_shuffle[int(mask_idx[0].shape[0] * 0.9):]\n",
    "\n",
    "    ## mask the tokens\n",
    "    mlm_data[mask_idx[0][tomask_idx], mask_idx[1][tomask_idx]] = mask_token_id\n",
    "    mlm_data[mask_idx[0][torandom_idx], mask_idx[1][torandom_idx]] = generate_random_token_ids((torandom_idx.shape[0],), vocab_size, pad_token_id, mask_token_id) #@ this mask token is not in the vocab\n",
    "\n",
    "    ## create the labels\n",
    "    labels[~mask] = -100\n",
    "\n",
    "    return mlm_data, labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Tensor...\n",
      "tensor([[14, 22, 29, 15,  8],\n",
      "        [26,  7, 17,  3, 18],\n",
      "        [ 8, 17,  4, 26, 12],\n",
      "        [23, 25, 26, 22, 15],\n",
      "        [28,  8, 13, 21,  6]])\n",
      "Masked Language Model...\n",
      "MLM data...\n",
      "tensor([[ 1,  6, 29, 15,  8],\n",
      "        [ 1,  7, 17,  3, 18],\n",
      "        [ 8, 17,  4,  1, 12],\n",
      "        [23, 25,  1, 22, 15],\n",
      "        [28,  8,  1, 21,  1]])\n",
      "Labels...\n",
      "tensor([[  14,   22, -100, -100, -100],\n",
      "        [  26, -100, -100, -100, -100],\n",
      "        [-100, -100, -100,   26, -100],\n",
      "        [-100, -100,   26, -100, -100],\n",
      "        [-100, -100,   13,   21,    6]])\n"
     ]
    }
   ],
   "source": [
    "input_tensor = torch.randint(2, 30, (5,5))\n",
    "print('Input Tensor...')\n",
    "print(input_tensor)\n",
    "\n",
    "print('Masked Language Model...')\n",
    "res = masked_lm(input_tensor, mlm_prob = 0.5, pad_token_id = 0, mask_token_id = 1, vocab_size = 30)\n",
    "print('MLM data...')    \n",
    "print(res[0])\n",
    "print('Labels...')\n",
    "print(res[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(assuming that you did not execute the above)\n",
    "\n",
    "From the above example, observe how the labels matrix retains the true values of the original input tensor for the indices that are masked. Indices that are not masked are placed as -100. This is so that the cross_entropy calculation will ignore the labels that have values -100. The main output is actually the X (the MLM data). Observe the following 3 groups.\n",
    "\n",
    "Group 1: indexes (0,0), (1,0), (2,3), (3,2), (4,2), (4,4) are masked as '1's\n",
    "Group 2: index (0,1) has been randomly replaced with a value that is not in the special tokens (6 -> 22)\n",
    "Group 3: index (4,3) has been kept as it is (21 stays as 21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "smallville",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
