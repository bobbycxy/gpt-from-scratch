{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figuring out the tokenizer update."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Hugging Face's Tokenizer Example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bobby/miniconda3/envs/smallville/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_ckpt = 'gpt2'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt,\n",
    "                                          cache_dir=\"/data/bobby/huggingface-cache/models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"This is the Hugging Face Course.\",\n",
    "    \"This chapter is about tokenization.\",\n",
    "    \"This section shows several tokenizer algorithms.\",\n",
    "    \"Hopefully, you will be able to understand how they are trained and generate tokens.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 1212,   318,   262, 12905,  2667, 15399, 20537,    13, 50257, 50257,\n",
       "         50257, 50257, 50257, 50257, 50257, 50257],\n",
       "        [ 1212,  6843,   318,   546, 11241,  1634,    13, 50257, 50257, 50257,\n",
       "         50257, 50257, 50257, 50257, 50257, 50257],\n",
       "        [ 1212,  2665,  2523,  1811, 11241,  7509, 16113,    13, 50257, 50257,\n",
       "         50257, 50257, 50257, 50257, 50257, 50257],\n",
       "        [32365,    11,   345,   481,   307,  1498,   284,  1833,   703,   484,\n",
       "           389,  8776,   290,  7716, 16326,    13]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.add_special_tokens({'pad_token': '<pad>'})\n",
    "res = tokenizer(corpus, padding=True, truncation=True, return_tensors='pt')\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This chapter is about tokenization.'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer.encode(corpus[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is the Hugging Face Course.<pad><pad><pad><pad><pad><pad><pad><pad>'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(res['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0184d4b7a5a48c9bad4b540f22b5fda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/35.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89b007b5f4e34a339230d5d72fed1483",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/649k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3b8427b6e65486b986c9d3fab89406e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/75.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f69752c176747eeac6ba994f8baefad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/308k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b1856ee1915494199234b5c6b4e192c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/3668 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a96e77cd4fdf4987b743a869bd1d700b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/408 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbb7fdc134164cf199c086621dbd6953",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/1725 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset('glue', 'mrpc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "      <th>label</th>\n",
       "      <th>idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Amrozi accused his brother , whom he called \" ...</td>\n",
       "      <td>Referring to him as only \" the witness \" , Amr...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Yucaipa owned Dominick 's before selling the c...</td>\n",
       "      <td>Yucaipa bought Dominick 's in 1995 for $ 693 m...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>They had published an advertisement on the Int...</td>\n",
       "      <td>On June 10 , the ship 's owners had published ...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Around 0335 GMT , Tab shares were up 19 cents ...</td>\n",
       "      <td>Tab shares jumped 20 cents , or 4.6 % , to set...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The stock rose $ 2.11 , or about 11 percent , ...</td>\n",
       "      <td>PG &amp; E Corp. shares jumped $ 1.63 or 8 percent...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3663</th>\n",
       "      <td>\" At this point , Mr. Brando announced : ' Som...</td>\n",
       "      <td>Brando said that \" somebody ought to put a bul...</td>\n",
       "      <td>1</td>\n",
       "      <td>4071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3664</th>\n",
       "      <td>Martin , 58 , will be freed today after servin...</td>\n",
       "      <td>Martin served two thirds of a five-year senten...</td>\n",
       "      <td>0</td>\n",
       "      <td>4072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3665</th>\n",
       "      <td>\" We have concluded that the outlook for price...</td>\n",
       "      <td>In a statement , the ECB said the outlook for ...</td>\n",
       "      <td>1</td>\n",
       "      <td>4073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3666</th>\n",
       "      <td>The notification was first reported Friday by ...</td>\n",
       "      <td>MSNBC.com first reported the CIA request on Fr...</td>\n",
       "      <td>1</td>\n",
       "      <td>4074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3667</th>\n",
       "      <td>The 30-year bond US30YT = RR rose 22 / 32 for ...</td>\n",
       "      <td>The 30-year bond US30YT = RR grew 1-3 / 32 for...</td>\n",
       "      <td>0</td>\n",
       "      <td>4075</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3668 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              sentence1  ...   idx\n",
       "0     Amrozi accused his brother , whom he called \" ...  ...     0\n",
       "1     Yucaipa owned Dominick 's before selling the c...  ...     1\n",
       "2     They had published an advertisement on the Int...  ...     2\n",
       "3     Around 0335 GMT , Tab shares were up 19 cents ...  ...     3\n",
       "4     The stock rose $ 2.11 , or about 11 percent , ...  ...     4\n",
       "...                                                 ...  ...   ...\n",
       "3663  \" At this point , Mr. Brando announced : ' Som...  ...  4071\n",
       "3664  Martin , 58 , will be freed today after servin...  ...  4072\n",
       "3665  \" We have concluded that the outlook for price...  ...  4073\n",
       "3666  The notification was first reported Friday by ...  ...  4074\n",
       "3667  The 30-year bond US30YT = RR rose 22 / 32 for ...  ...  4075\n",
       "\n",
       "[3668 rows x 4 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame(dataset['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Self Created Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"This is the Hugging Face Course.\",\n",
    "    \"This chapter is about tokenization.\",\n",
    "    \"This section shows several tokenizer algorithms.\",\n",
    "    \"Hopefully, you will be able to understand how they are trained and generate tokens.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text: <pad> hello world [CLS] test [SEP]\n",
      "Tokens: ['<pad>', ' hello', ' world', ' ', '[CLS]', ' test', ' ', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def pre_tokenize(text, special_tokens):\n",
    "    # Escape special tokens for regex\n",
    "    escaped_tokens = [re.escape(token) for token in special_tokens]\n",
    "    # Create a regex pattern to match special tokens\n",
    "    special_tokens_pattern = '|'.join(escaped_tokens)\n",
    "    \n",
    "    # Split text using the pattern, keeping special tokens intact\n",
    "    parts = re.split(f'({special_tokens_pattern})', text)\n",
    "    \n",
    "    # Process parts to combine spaces with non-special tokens\n",
    "    tokens = []\n",
    "    for part in parts:\n",
    "        if part in special_tokens:\n",
    "            tokens.append(part)\n",
    "        else:\n",
    "            # Find words and spaces, and combine them\n",
    "            sub_tokens = re.findall(r'\\s*\\S+|\\s+', part)\n",
    "            tokens.extend(sub_tokens)\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "# Example usage\n",
    "input_text = \"<pad> hello world [CLS] test [SEP]\"\n",
    "\n",
    "special_tokens = ['<pad>', '[CLS]', '[SEP]', '[MASK]']\n",
    "\n",
    "tokens = pre_tokenize(input_text, special_tokens)\n",
    "\n",
    "print(\"Input text:\", input_text)\n",
    "print(\"Tokens:\", tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<pad>', ' hello', ' world', ' ', '[CLS]', ' test', ' ', '[SEP]']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_tokenize(input_text,['<pad>', '[CLS]', '[SEP]', '[MASK]'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {'This': 3, ' is': 2, ' the': 1, ' Hugging': 1, ' Face': 1, ' Course.': 1, ' chapter': 1, ' about': 1, ' tokenization.': 1, ' section': 1, ' shows': 1, ' several': 1, ' tokenizer': 1, ' algorithms.': 1, 'Hopefully,': 1, ' you': 1, ' will': 1, ' be': 1, ' able': 1, ' to': 1, ' understand': 1, ' how': 1, ' they': 1, ' are': 1, ' trained': 1, ' and': 1, ' generate': 1, ' tokens.': 1})\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "word_freq = defaultdict(int)\n",
    "\n",
    "for text in corpus:\n",
    "    words_with_offsets = pre_tokenize(text,special_tokens)\n",
    "    new_words = [word for word in words_with_offsets]\n",
    "    for word in new_words:\n",
    "        word_freq[word] += 1\n",
    "\n",
    "print(word_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ', ',', '.', 'C', 'F', 'H', 'T', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'y', 'z']\n"
     ]
    }
   ],
   "source": [
    "alphabet = []\n",
    "\n",
    "for word in word_freq.keys():\n",
    "    for letter in word:\n",
    "        if letter not in alphabet:\n",
    "            alphabet.append(letter)\n",
    "\n",
    "alphabet.sort()\n",
    "\n",
    "print(alphabet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<pad>', ' ', ',', '.', 'C', 'F', 'H', 'T', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'y', 'z']\n"
     ]
    }
   ],
   "source": [
    "vocab = ['<pad>'] + alphabet.copy()\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'This': ['T', 'h', 'i', 's'],\n",
       " ' is': [' ', 'i', 's'],\n",
       " ' the': [' ', 't', 'h', 'e'],\n",
       " ' Hugging': [' ', 'H', 'u', 'g', 'g', 'i', 'n', 'g'],\n",
       " ' Face': [' ', 'F', 'a', 'c', 'e'],\n",
       " ' Course.': [' ', 'C', 'o', 'u', 'r', 's', 'e', '.'],\n",
       " ' chapter': [' ', 'c', 'h', 'a', 'p', 't', 'e', 'r'],\n",
       " ' about': [' ', 'a', 'b', 'o', 'u', 't'],\n",
       " ' tokenization.': [' ',\n",
       "  't',\n",
       "  'o',\n",
       "  'k',\n",
       "  'e',\n",
       "  'n',\n",
       "  'i',\n",
       "  'z',\n",
       "  'a',\n",
       "  't',\n",
       "  'i',\n",
       "  'o',\n",
       "  'n',\n",
       "  '.'],\n",
       " ' section': [' ', 's', 'e', 'c', 't', 'i', 'o', 'n'],\n",
       " ' shows': [' ', 's', 'h', 'o', 'w', 's'],\n",
       " ' several': [' ', 's', 'e', 'v', 'e', 'r', 'a', 'l'],\n",
       " ' tokenizer': [' ', 't', 'o', 'k', 'e', 'n', 'i', 'z', 'e', 'r'],\n",
       " ' algorithms.': [' ', 'a', 'l', 'g', 'o', 'r', 'i', 't', 'h', 'm', 's', '.'],\n",
       " 'Hopefully,': ['H', 'o', 'p', 'e', 'f', 'u', 'l', 'l', 'y', ','],\n",
       " ' you': [' ', 'y', 'o', 'u'],\n",
       " ' will': [' ', 'w', 'i', 'l', 'l'],\n",
       " ' be': [' ', 'b', 'e'],\n",
       " ' able': [' ', 'a', 'b', 'l', 'e'],\n",
       " ' to': [' ', 't', 'o'],\n",
       " ' understand': [' ', 'u', 'n', 'd', 'e', 'r', 's', 't', 'a', 'n', 'd'],\n",
       " ' how': [' ', 'h', 'o', 'w'],\n",
       " ' they': [' ', 't', 'h', 'e', 'y'],\n",
       " ' are': [' ', 'a', 'r', 'e'],\n",
       " ' trained': [' ', 't', 'r', 'a', 'i', 'n', 'e', 'd'],\n",
       " ' and': [' ', 'a', 'n', 'd'],\n",
       " ' generate': [' ', 'g', 'e', 'n', 'e', 'r', 'a', 't', 'e'],\n",
       " ' tokens.': [' ', 't', 'o', 'k', 'e', 'n', 's', '.']}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## time to split the words into individual characters before \n",
    "## performing the BPE tokenization\n",
    "\n",
    "splits = {word: [c for c in word] for word in word_freq.keys()}\n",
    "splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pair_freqs(splits):\n",
    "    pair_freqs = defaultdict(int)\n",
    "    for word, freq in word_freq.items():\n",
    "        chars = splits[word]\n",
    "        if len(chars) == 1:\n",
    "            continue\n",
    "        for i in range(len(chars) - 1):\n",
    "            pair_freqs[(chars[i], chars[i + 1])] += freq\n",
    "    return pair_freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_freqs = compute_pair_freqs(splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('T', 'h'): 3\n",
      "('h', 'i'): 3\n",
      "('i', 's'): 5\n",
      "(' ', 'i'): 2\n",
      "(' ', 't'): 7\n",
      "('t', 'h'): 3\n"
     ]
    }
   ],
   "source": [
    "for i, key in enumerate(pair_freqs.keys()):\n",
    "    print(f'{key}: {pair_freqs[key]}')\n",
    "    if i >= 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(' ', 't') 7\n"
     ]
    }
   ],
   "source": [
    "best_pair = \"\"\n",
    "max_freq = None\n",
    "\n",
    "for pair, freq in pair_freqs.items():\n",
    "    if max_freq is None or max_freq < freq:\n",
    "        best_pair = pair\n",
    "        max_freq = freq\n",
    "\n",
    "print(best_pair, max_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_pair(a, b, splits):\n",
    "    for word in word_freq:\n",
    "        split = splits[word]\n",
    "        if len(split) == 1:\n",
    "            continue\n",
    "\n",
    "        i = 0\n",
    "        while i < len(split) - 1:\n",
    "            if split[i] == a and split[i + 1] == b:\n",
    "                split = split[:i] + [a + b] + split[i + 2:]\n",
    "            else:\n",
    "                i += 1\n",
    "        splits[word] = split\n",
    "    return splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create a vocab size of 50\n",
    "vocab_size = 100\n",
    "merges = {}\n",
    "\n",
    "while len(vocab) < vocab_size:\n",
    "    pair_freqs = compute_pair_freqs(splits)\n",
    "    best_pair = \"\"\n",
    "    max_freq = None\n",
    "    for pair, freq in pair_freqs.items():\n",
    "        if max_freq is None or max_freq < freq:\n",
    "            best_pair = pair\n",
    "            max_freq = freq\n",
    "    splits = merge_pair(best_pair[0], best_pair[1], splits)\n",
    "    merges[best_pair] = best_pair[0] + best_pair[1]\n",
    "    vocab.append(best_pair[0] + best_pair[1])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(' ', 't'): ' t',\n",
       " ('i', 's'): 'is',\n",
       " ('e', 'r'): 'er',\n",
       " (' ', 'a'): ' a',\n",
       " (' t', 'o'): ' to',\n",
       " ('e', 'n'): 'en',\n",
       " ('T', 'h'): 'Th',\n",
       " ('Th', 'is'): 'This',\n",
       " ('o', 'u'): 'ou',\n",
       " ('s', 'e'): 'se',\n",
       " (' to', 'k'): ' tok',\n",
       " (' tok', 'en'): ' token',\n",
       " ('n', 'd'): 'nd',\n",
       " (' ', 'is'): ' is',\n",
       " (' t', 'h'): ' th',\n",
       " (' th', 'e'): ' the',\n",
       " ('i', 'n'): 'in',\n",
       " (' a', 'b'): ' ab',\n",
       " (' token', 'i'): ' tokeni',\n",
       " (' tokeni', 'z'): ' tokeniz',\n",
       " ('a', 't'): 'at',\n",
       " ('i', 'o'): 'io',\n",
       " ('io', 'n'): 'ion',\n",
       " (' ', 'se'): ' se',\n",
       " ('h', 'o'): 'ho',\n",
       " ('ho', 'w'): 'how',\n",
       " ('s', '.'): 's.',\n",
       " ('l', 'l'): 'll',\n",
       " (' ', 'H'): ' H',\n",
       " (' H', 'u'): ' Hu',\n",
       " (' Hu', 'g'): ' Hug',\n",
       " (' Hug', 'g'): ' Hugg',\n",
       " (' Hugg', 'in'): ' Huggin',\n",
       " (' Huggin', 'g'): ' Hugging',\n",
       " (' ', 'F'): ' F',\n",
       " (' F', 'a'): ' Fa',\n",
       " (' Fa', 'c'): ' Fac',\n",
       " (' Fac', 'e'): ' Face',\n",
       " (' ', 'C'): ' C',\n",
       " (' C', 'ou'): ' Cou',\n",
       " (' Cou', 'r'): ' Cour',\n",
       " (' Cour', 'se'): ' Course',\n",
       " (' Course', '.'): ' Course.',\n",
       " (' ', 'c'): ' c',\n",
       " (' c', 'h'): ' ch',\n",
       " (' ch', 'a'): ' cha',\n",
       " (' cha', 'p'): ' chap',\n",
       " (' chap', 't'): ' chapt',\n",
       " (' chapt', 'er'): ' chapter',\n",
       " (' ab', 'ou'): ' abou',\n",
       " (' abou', 't'): ' about',\n",
       " (' tokeniz', 'at'): ' tokenizat',\n",
       " (' tokenizat', 'ion'): ' tokenization',\n",
       " (' tokenization', '.'): ' tokenization.',\n",
       " (' se', 'c'): ' sec',\n",
       " (' sec', 't'): ' sect',\n",
       " (' sect', 'ion'): ' section',\n",
       " (' ', 's'): ' s',\n",
       " (' s', 'how'): ' show',\n",
       " (' show', 's'): ' shows',\n",
       " (' se', 'v'): ' sev',\n",
       " (' sev', 'er'): ' sever',\n",
       " (' sever', 'a'): ' severa',\n",
       " (' severa', 'l'): ' several',\n",
       " (' tokeniz', 'er'): ' tokenizer',\n",
       " (' a', 'l'): ' al',\n",
       " (' al', 'g'): ' alg',\n",
       " (' alg', 'o'): ' algo',\n",
       " (' algo', 'r'): ' algor'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<pad>', ' ', ',', '.', 'C', 'F', 'H', 'T', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'y', 'z', ' t', 'is', 'er', ' a', ' to', 'en', 'Th', 'This', 'ou', 'se', ' tok', ' token', 'nd', ' is', ' th', ' the', 'in', ' ab', ' tokeni', ' tokeniz', 'at', 'io', 'ion', ' se', 'ho', 'how', 's.', 'll', ' H', ' Hu', ' Hug', ' Hugg', ' Huggin', ' Hugging', ' F', ' Fa', ' Fac', ' Face', ' C', ' Cou', ' Cour', ' Course', ' Course.', ' c', ' ch', ' cha', ' chap', ' chapt', ' chapter', ' abou', ' about', ' tokenizat', ' tokenization', ' tokenization.', ' sec', ' sect', ' section', ' s', ' show', ' shows', ' sev', ' sever', ' severa', ' several', ' tokenizer', ' al', ' alg', ' algo', ' algor']\n"
     ]
    }
   ],
   "source": [
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['H', 'e', 'll', 'o'],\n",
       " [' '],\n",
       " ['<pad>'],\n",
       " [' ', 'w', 'o', 'r', 'l', 'd', '!'],\n",
       " [' ', 'This'],\n",
       " [' is'],\n",
       " [' ', 'n', 'o', 't'],\n",
       " [' a'],\n",
       " [' tokenizer', '!']]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'Hello <pad> world! This is not a tokenizer!'\n",
    "\n",
    "pre_tokenize_result = pre_tokenize(text, ['<pad>'])\n",
    "pre_tokenized_text  = [word for word in pre_tokenize_result]\n",
    "splits =[[word] if any(substring in word for substring in ['<pad>']) else [l for l in word] for word in pre_tokenized_text]\n",
    "\n",
    "for pair, merge in merges.items():\n",
    "    for idx, split in enumerate(splits):\n",
    "        i = 0\n",
    "        while i < len(split) - 1:\n",
    "            if split[i] == pair[0] and split[i + 1] == pair[1]:\n",
    "                split = split[:i] + [merge] + split[i + 2:]\n",
    "            else:\n",
    "                i += 1\n",
    "        splits[idx] = split\n",
    "\n",
    "splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello <pad> world! This is not a tokenizer!\n"
     ]
    }
   ],
   "source": [
    "print(''.join(sum(splits,[])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello', ' ', '<pad>', ' world!', ' This', ' is', ' not', ' a', ' tokenizer!']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_tokenize(text, ['<pad>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "\n",
    "    pre_tokenize_result = pre_tokenize(text, special_tokens)\n",
    "    pre_tokenized_text  = [word for word in pre_tokenize_result]\n",
    "    splits =[[word] if any(substring in word for substring in ['<pad>']) else [l for l in word] for word in pre_tokenized_text]\n",
    "\n",
    "    for pair, merge in merges.items():\n",
    "        for idx, split in enumerate(splits):\n",
    "            i = 0\n",
    "            while i < len(split) - 1:\n",
    "                if split[i] == pair[0] and split[i + 1] == pair[1]:\n",
    "                    split = split[:i] + [merge] + split[i + 2:]\n",
    "                else:\n",
    "                    i += 1\n",
    "            splits[idx] = split\n",
    "\n",
    "    return sum(splits,[])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['H', 'e', 'll', 'o', ' ', '<pad>', ' ', 'w', 'o', 'r', 'l', 'd', '.', ' ', 'This', ' is', ' ', 'n', 'o', 't', ' a', ' tokenizer', '.']\n"
     ]
    }
   ],
   "source": [
    "print(tokenize('Hello <pad> world. This is not a tokenizer.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "chtoi = {ch:i for i, ch in enumerate(vocab)}\n",
    "itoch = {i:ch for i, ch in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6,\n",
       " 12,\n",
       " 58,\n",
       " 21,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 28,\n",
       " 21,\n",
       " 23,\n",
       " 18,\n",
       " 11,\n",
       " 3,\n",
       " 1,\n",
       " 38,\n",
       " 44,\n",
       " 1,\n",
       " 20,\n",
       " 21,\n",
       " 25,\n",
       " 34,\n",
       " 95,\n",
       " 3]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[chtoi[ch] for ch in tokenize('Hello <pad> world. This is not a tokenizer.')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello <pad> world. This is not a tokenizer.'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''.join([itoch[i] for i in [chtoi[ch] for ch in tokenize('Hello <pad> world. This is not a tokenizer.')]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6,\n",
       " 12,\n",
       " 58,\n",
       " 21,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 28,\n",
       " 21,\n",
       " 23,\n",
       " 18,\n",
       " 11,\n",
       " 3,\n",
       " 1,\n",
       " 38,\n",
       " 44,\n",
       " 1,\n",
       " 20,\n",
       " 21,\n",
       " 25,\n",
       " 34,\n",
       " 95,\n",
       " 3]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[chtoi[ch] for ch in tokenize('Hello <pad> world. This is not a tokenizer.')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = [chtoi[ch] for ch in tokenize('Hello <pad> world. This is not a tokenizer.')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.where(torch.tensor(sample) == 0, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### now write a python function that will return 2 things\n",
    "### 1. the tokenized text e.g. [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "### 2. the attention mask e.g. [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
    "### 3. also create a decoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text: <pad> hello world [CLS] test [SEP]\n",
      "Tokens: ['<pad>', ' hello', ' world', ' ', '[CLS]', ' test', ' ', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def pre_tokenize(text, special_tokens):\n",
    "    # Escape special tokens for regex\n",
    "    escaped_tokens = [re.escape(token) for token in special_tokens]\n",
    "    # Create a regex pattern to match special tokens\n",
    "    special_tokens_pattern = '|'.join(escaped_tokens)\n",
    "    \n",
    "    # Split text using the pattern, keeping special tokens intact\n",
    "    parts = re.split(f'({special_tokens_pattern})', text)\n",
    "    \n",
    "    # Process parts to combine spaces with non-special tokens\n",
    "    tokens = []\n",
    "    for part in parts:\n",
    "        if part in special_tokens:\n",
    "            tokens.append(part)\n",
    "        else:\n",
    "            # Find words and spaces, and combine them\n",
    "            sub_tokens = re.findall(r'\\s*\\S+|\\s+', part)\n",
    "            tokens.extend(sub_tokens)\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "# Example usage\n",
    "input_text = \"<pad> hello world [CLS] test [SEP]\"\n",
    "\n",
    "tokens = pre_tokenize(input_text, special_tokens)\n",
    "\n",
    "print(\"Input text:\", input_text)\n",
    "print(\"Tokens:\", tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"This is the Hugging Face Course.\",\n",
    "    \"This chapter is about tokenization.\",\n",
    "    \"This section shows several tokenizer algorithms.\",\n",
    "    \"Hopefully, you will be able to understand how they are trained and generate tokens.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bobby/miniconda3/envs/smallville/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d03fa6c9394a4a6b9783b0ede502bf76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21a5c2427051402b9c406bc28ad8d0a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0f3bd5dccf6416c969fae859f60f95c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ff6abcedbb84aed9201f486d607a15f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01781e9cbbad49819095039eb2063dce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\",\n",
    "                                          cache_dir = \"/data/bobby/huggingface-cache/models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {'This': 3, 'Ġis': 2, 'Ġthe': 1, 'ĠHugging': 1, 'ĠFace': 1, 'ĠCourse': 1, '.': 4, 'Ġchapter': 1, 'Ġabout': 1, 'Ġtokenization': 1, 'Ġsection': 1, 'Ġshows': 1, 'Ġseveral': 1, 'Ġtokenizer': 1, 'Ġalgorithms': 1, 'Hopefully': 1, ',': 1, 'Ġyou': 1, 'Ġwill': 1, 'Ġbe': 1, 'Ġable': 1, 'Ġto': 1, 'Ġunderstand': 1, 'Ġhow': 1, 'Ġthey': 1, 'Ġare': 1, 'Ġtrained': 1, 'Ġand': 1, 'Ġgenerate': 1, 'Ġtokens': 1})\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "word_freq = defaultdict(int)\n",
    "\n",
    "for text in corpus:\n",
    "    words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
    "    new_words = [word for word, offset in words_with_offsets]\n",
    "    for word in new_words:\n",
    "        word_freq[word] += 1\n",
    "\n",
    "print(word_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[',', '.', 'C', 'F', 'H', 'T', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'y', 'z', 'Ġ']\n"
     ]
    }
   ],
   "source": [
    "alphabet = []\n",
    "\n",
    "for word in word_freq.keys():\n",
    "    for letter in word:\n",
    "        if letter not in alphabet:\n",
    "            alphabet.append(letter)\n",
    "\n",
    "alphabet.sort()\n",
    "\n",
    "print(alphabet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|endoftext|>', ',', '.', 'C', 'F', 'H', 'T', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'y', 'z', 'Ġ']\n"
     ]
    }
   ],
   "source": [
    "vocab = ['<|endoftext|>'] + alphabet.copy()\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'This': ['T', 'h', 'i', 's'],\n",
       " 'Ġis': ['Ġ', 'i', 's'],\n",
       " 'Ġthe': ['Ġ', 't', 'h', 'e'],\n",
       " 'ĠHugging': ['Ġ', 'H', 'u', 'g', 'g', 'i', 'n', 'g'],\n",
       " 'ĠFace': ['Ġ', 'F', 'a', 'c', 'e'],\n",
       " 'ĠCourse': ['Ġ', 'C', 'o', 'u', 'r', 's', 'e'],\n",
       " '.': ['.'],\n",
       " 'Ġchapter': ['Ġ', 'c', 'h', 'a', 'p', 't', 'e', 'r'],\n",
       " 'Ġabout': ['Ġ', 'a', 'b', 'o', 'u', 't'],\n",
       " 'Ġtokenization': ['Ġ',\n",
       "  't',\n",
       "  'o',\n",
       "  'k',\n",
       "  'e',\n",
       "  'n',\n",
       "  'i',\n",
       "  'z',\n",
       "  'a',\n",
       "  't',\n",
       "  'i',\n",
       "  'o',\n",
       "  'n'],\n",
       " 'Ġsection': ['Ġ', 's', 'e', 'c', 't', 'i', 'o', 'n'],\n",
       " 'Ġshows': ['Ġ', 's', 'h', 'o', 'w', 's'],\n",
       " 'Ġseveral': ['Ġ', 's', 'e', 'v', 'e', 'r', 'a', 'l'],\n",
       " 'Ġtokenizer': ['Ġ', 't', 'o', 'k', 'e', 'n', 'i', 'z', 'e', 'r'],\n",
       " 'Ġalgorithms': ['Ġ', 'a', 'l', 'g', 'o', 'r', 'i', 't', 'h', 'm', 's'],\n",
       " 'Hopefully': ['H', 'o', 'p', 'e', 'f', 'u', 'l', 'l', 'y'],\n",
       " ',': [','],\n",
       " 'Ġyou': ['Ġ', 'y', 'o', 'u'],\n",
       " 'Ġwill': ['Ġ', 'w', 'i', 'l', 'l'],\n",
       " 'Ġbe': ['Ġ', 'b', 'e'],\n",
       " 'Ġable': ['Ġ', 'a', 'b', 'l', 'e'],\n",
       " 'Ġto': ['Ġ', 't', 'o'],\n",
       " 'Ġunderstand': ['Ġ', 'u', 'n', 'd', 'e', 'r', 's', 't', 'a', 'n', 'd'],\n",
       " 'Ġhow': ['Ġ', 'h', 'o', 'w'],\n",
       " 'Ġthey': ['Ġ', 't', 'h', 'e', 'y'],\n",
       " 'Ġare': ['Ġ', 'a', 'r', 'e'],\n",
       " 'Ġtrained': ['Ġ', 't', 'r', 'a', 'i', 'n', 'e', 'd'],\n",
       " 'Ġand': ['Ġ', 'a', 'n', 'd'],\n",
       " 'Ġgenerate': ['Ġ', 'g', 'e', 'n', 'e', 'r', 'a', 't', 'e'],\n",
       " 'Ġtokens': ['Ġ', 't', 'o', 'k', 'e', 'n', 's']}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## time to split the words into individual characters before \n",
    "## performing the BPE tokenization\n",
    "\n",
    "splits = {word: [c for c in word] for word in word_freq.keys()}\n",
    "splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pair_freqs(splits):\n",
    "    pair_freqs = defaultdict(int)\n",
    "    for word, freq in word_freq.items():\n",
    "        chars = splits[word]\n",
    "        if len(chars) == 1:\n",
    "            continue\n",
    "        for i in range(len(chars) - 1):\n",
    "            pair_freqs[(chars[i], chars[i + 1])] += freq\n",
    "    return pair_freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_freqs = compute_pair_freqs(splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('T', 'h'): 3\n",
      "('h', 'i'): 3\n",
      "('i', 's'): 5\n",
      "('Ġ', 'i'): 2\n",
      "('Ġ', 't'): 7\n",
      "('t', 'h'): 3\n"
     ]
    }
   ],
   "source": [
    "for i, key in enumerate(pair_freqs.keys()):\n",
    "    print(f'{key}: {pair_freqs[key]}')\n",
    "    if i >= 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Ġ', 't') 7\n"
     ]
    }
   ],
   "source": [
    "best_pair = \"\"\n",
    "max_freq = None\n",
    "\n",
    "for pair, freq in pair_freqs.items():\n",
    "    if max_freq is None or max_freq < freq:\n",
    "        best_pair = pair\n",
    "        max_freq = freq\n",
    "\n",
    "print(best_pair, max_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_pair(a, b, splits):\n",
    "    for word in word_freq:\n",
    "        split = splits[word]\n",
    "        if len(split) == 1:\n",
    "            continue\n",
    "\n",
    "        i = 0\n",
    "        while i < len(split) - 1:\n",
    "            if split[i] == a and split[i + 1] == b:\n",
    "                split = split[:i] + [a + b] + split[i + 2:]\n",
    "            else:\n",
    "                i += 1\n",
    "        splits[word] = split\n",
    "    return splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'This': ['T', 'h', 'i', 's'],\n",
       " 'Ġis': ['Ġ', 'i', 's'],\n",
       " 'Ġthe': ['Ġt', 'h', 'e'],\n",
       " 'ĠHugging': ['Ġ', 'H', 'u', 'g', 'g', 'i', 'n', 'g'],\n",
       " 'ĠFace': ['Ġ', 'F', 'a', 'c', 'e'],\n",
       " 'ĠCourse': ['Ġ', 'C', 'o', 'u', 'r', 's', 'e'],\n",
       " '.': ['.'],\n",
       " 'Ġchapter': ['Ġ', 'c', 'h', 'a', 'p', 't', 'e', 'r'],\n",
       " 'Ġabout': ['Ġ', 'a', 'b', 'o', 'u', 't'],\n",
       " 'Ġtokenization': ['Ġt',\n",
       "  'o',\n",
       "  'k',\n",
       "  'e',\n",
       "  'n',\n",
       "  'i',\n",
       "  'z',\n",
       "  'a',\n",
       "  't',\n",
       "  'i',\n",
       "  'o',\n",
       "  'n'],\n",
       " 'Ġsection': ['Ġ', 's', 'e', 'c', 't', 'i', 'o', 'n'],\n",
       " 'Ġshows': ['Ġ', 's', 'h', 'o', 'w', 's'],\n",
       " 'Ġseveral': ['Ġ', 's', 'e', 'v', 'e', 'r', 'a', 'l'],\n",
       " 'Ġtokenizer': ['Ġt', 'o', 'k', 'e', 'n', 'i', 'z', 'e', 'r'],\n",
       " 'Ġalgorithms': ['Ġ', 'a', 'l', 'g', 'o', 'r', 'i', 't', 'h', 'm', 's'],\n",
       " 'Hopefully': ['H', 'o', 'p', 'e', 'f', 'u', 'l', 'l', 'y'],\n",
       " ',': [','],\n",
       " 'Ġyou': ['Ġ', 'y', 'o', 'u'],\n",
       " 'Ġwill': ['Ġ', 'w', 'i', 'l', 'l'],\n",
       " 'Ġbe': ['Ġ', 'b', 'e'],\n",
       " 'Ġable': ['Ġ', 'a', 'b', 'l', 'e'],\n",
       " 'Ġto': ['Ġt', 'o'],\n",
       " 'Ġunderstand': ['Ġ', 'u', 'n', 'd', 'e', 'r', 's', 't', 'a', 'n', 'd'],\n",
       " 'Ġhow': ['Ġ', 'h', 'o', 'w'],\n",
       " 'Ġthey': ['Ġt', 'h', 'e', 'y'],\n",
       " 'Ġare': ['Ġ', 'a', 'r', 'e'],\n",
       " 'Ġtrained': ['Ġt', 'r', 'a', 'i', 'n', 'e', 'd'],\n",
       " 'Ġand': ['Ġ', 'a', 'n', 'd'],\n",
       " 'Ġgenerate': ['Ġ', 'g', 'e', 'n', 'e', 'r', 'a', 't', 'e'],\n",
       " 'Ġtokens': ['Ġt', 'o', 'k', 'e', 'n', 's']}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merge_pair(\"Ġ\", \"t\", splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create a vocab size of 50\n",
    "vocab_size = 50\n",
    "merges = {}\n",
    "\n",
    "while len(vocab) < vocab_size:\n",
    "    pair_freqs = compute_pair_freqs(splits)\n",
    "    best_pair = \"\"\n",
    "    max_freq = None\n",
    "    for pair, freq in pair_freqs.items():\n",
    "        if max_freq is None or max_freq < freq:\n",
    "            best_pair = pair\n",
    "            max_freq = freq\n",
    "    splits = merge_pair(best_pair[0], best_pair[1], splits)\n",
    "    merges[best_pair] = best_pair[0] + best_pair[1]\n",
    "    vocab.append(best_pair[0] + best_pair[1])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('i', 's'): 'is',\n",
       " ('e', 'r'): 'er',\n",
       " ('Ġ', 'a'): 'Ġa',\n",
       " ('Ġt', 'o'): 'Ġto',\n",
       " ('e', 'n'): 'en',\n",
       " ('T', 'h'): 'Th',\n",
       " ('Th', 'is'): 'This',\n",
       " ('o', 'u'): 'ou',\n",
       " ('s', 'e'): 'se',\n",
       " ('Ġto', 'k'): 'Ġtok',\n",
       " ('Ġtok', 'en'): 'Ġtoken',\n",
       " ('n', 'd'): 'nd',\n",
       " ('Ġ', 'is'): 'Ġis',\n",
       " ('Ġt', 'h'): 'Ġth',\n",
       " ('Ġth', 'e'): 'Ġthe',\n",
       " ('i', 'n'): 'in',\n",
       " ('Ġa', 'b'): 'Ġab',\n",
       " ('Ġtoken', 'i'): 'Ġtokeni',\n",
       " ('Ġtokeni', 'z'): 'Ġtokeniz'}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|endoftext|>', ',', '.', 'C', 'F', 'H', 'T', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'y', 'z', 'Ġ', 'is', 'er', 'Ġa', 'Ġto', 'en', 'Th', 'This', 'ou', 'se', 'Ġtok', 'Ġtoken', 'nd', 'Ġis', 'Ġth', 'Ġthe', 'in', 'Ġab', 'Ġtokeni', 'Ġtokeniz']\n"
     ]
    }
   ],
   "source": [
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'This': ['This'],\n",
       " 'Ġis': ['Ġis'],\n",
       " 'Ġthe': ['Ġthe'],\n",
       " 'ĠHugging': ['Ġ', 'H', 'u', 'g', 'g', 'in', 'g'],\n",
       " 'ĠFace': ['Ġ', 'F', 'a', 'c', 'e'],\n",
       " 'ĠCourse': ['Ġ', 'C', 'ou', 'r', 'se'],\n",
       " '.': ['.'],\n",
       " 'Ġchapter': ['Ġ', 'c', 'h', 'a', 'p', 't', 'er'],\n",
       " 'Ġabout': ['Ġab', 'ou', 't'],\n",
       " 'Ġtokenization': ['Ġtokeniz', 'a', 't', 'i', 'o', 'n'],\n",
       " 'Ġsection': ['Ġ', 'se', 'c', 't', 'i', 'o', 'n'],\n",
       " 'Ġshows': ['Ġ', 's', 'h', 'o', 'w', 's'],\n",
       " 'Ġseveral': ['Ġ', 'se', 'v', 'er', 'a', 'l'],\n",
       " 'Ġtokenizer': ['Ġtokeniz', 'er'],\n",
       " 'Ġalgorithms': ['Ġa', 'l', 'g', 'o', 'r', 'i', 't', 'h', 'm', 's'],\n",
       " 'Hopefully': ['H', 'o', 'p', 'e', 'f', 'u', 'l', 'l', 'y'],\n",
       " ',': [','],\n",
       " 'Ġyou': ['Ġ', 'y', 'ou'],\n",
       " 'Ġwill': ['Ġ', 'w', 'i', 'l', 'l'],\n",
       " 'Ġbe': ['Ġ', 'b', 'e'],\n",
       " 'Ġable': ['Ġab', 'l', 'e'],\n",
       " 'Ġto': ['Ġto'],\n",
       " 'Ġunderstand': ['Ġ', 'u', 'nd', 'er', 's', 't', 'a', 'nd'],\n",
       " 'Ġhow': ['Ġ', 'h', 'o', 'w'],\n",
       " 'Ġthey': ['Ġthe', 'y'],\n",
       " 'Ġare': ['Ġa', 'r', 'e'],\n",
       " 'Ġtrained': ['Ġt', 'r', 'a', 'in', 'e', 'd'],\n",
       " 'Ġand': ['Ġa', 'nd'],\n",
       " 'Ġgenerate': ['Ġ', 'g', 'en', 'er', 'a', 't', 'e'],\n",
       " 'Ġtokens': ['Ġtoken', 's']}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'This is not a token <|endoftext|>'\n",
    "\n",
    "pre_tokenize_result = tokenizer._tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
    "pre_tokenized_text  = [word for word, offset in pre_tokenize_result]\n",
    "splits = [[l for l in word] for word in pre_tokenized_text]\n",
    "\n",
    "for pair, merge in merges.items():\n",
    "    for idx, split in enumerate(splits):\n",
    "        i = 0\n",
    "        while i < len(split) - 1:\n",
    "            if split[i] == pair[0] and split[i + 1] == pair[1]:\n",
    "                split = split[:i] + [merge] + split[i + 2:]\n",
    "            else:\n",
    "                i += 1\n",
    "        splits[idx] = split\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['This'],\n",
       " ['Ġis'],\n",
       " ['Ġ', 'n', 'o', 't'],\n",
       " ['Ġa'],\n",
       " ['Ġ', 't', 'o', 'k', 'en'],\n",
       " ['Ġ', '<', '|'],\n",
       " ['en', 'd', 'o', 'f', 't', 'e', 'x', 't'],\n",
       " ['|', '>']]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<|endoftext|>',\n",
       " ',',\n",
       " '.',\n",
       " 'C',\n",
       " 'F',\n",
       " 'H',\n",
       " 'T',\n",
       " 'a',\n",
       " 'b',\n",
       " 'c',\n",
       " 'd',\n",
       " 'e',\n",
       " 'f',\n",
       " 'g',\n",
       " 'h',\n",
       " 'i',\n",
       " 'k',\n",
       " 'l',\n",
       " 'm',\n",
       " 'n',\n",
       " 'o',\n",
       " 'p',\n",
       " 'r',\n",
       " 's',\n",
       " 't',\n",
       " 'u',\n",
       " 'v',\n",
       " 'w',\n",
       " 'y',\n",
       " 'z',\n",
       " 'Ġ',\n",
       " 'is',\n",
       " 'er',\n",
       " 'Ġa',\n",
       " 'Ġto',\n",
       " 'en',\n",
       " 'Th',\n",
       " 'This',\n",
       " 'ou',\n",
       " 'se',\n",
       " 'Ġtok',\n",
       " 'Ġtoken',\n",
       " 'nd',\n",
       " 'Ġis',\n",
       " 'Ġth',\n",
       " 'Ġthe',\n",
       " 'in',\n",
       " 'Ġab',\n",
       " 'Ġtokeni',\n",
       " 'Ġtokeniz']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('This', (0, 4)),\n",
       " ('Ġis', (4, 7)),\n",
       " ('Ġnot', (7, 11)),\n",
       " ('Ġa', (11, 13)),\n",
       " ('Ġtoken', (13, 19)),\n",
       " ('Ġ<|', (19, 22)),\n",
       " ('endoftext', (22, 31)),\n",
       " ('|>', (31, 33))]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_tokenize_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<pad>',\n",
       " ',',\n",
       " '.',\n",
       " 'C',\n",
       " 'F',\n",
       " 'H',\n",
       " 'T',\n",
       " 'a',\n",
       " 'b',\n",
       " 'c',\n",
       " 'd',\n",
       " 'e',\n",
       " 'f',\n",
       " 'g',\n",
       " 'h',\n",
       " 'i',\n",
       " 'k',\n",
       " 'l',\n",
       " 'm',\n",
       " 'n',\n",
       " 'o',\n",
       " 'p',\n",
       " 'r',\n",
       " 's',\n",
       " 't',\n",
       " 'u',\n",
       " 'v',\n",
       " 'w',\n",
       " 'y',\n",
       " 'z',\n",
       " 'Ġ']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Define special tokens\n",
    "special_tokens = ['<pad>']\n",
    "\n",
    "# Define basic vocabulary including special tokens and alphabets\n",
    "vocab = special_tokens + alphabet.copy()\n",
    "\n",
    "vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_tokenize(text, special_tokens):\n",
    "    # Escape special tokens for regex\n",
    "    escaped_tokens = [re.escape(token) for token in special_tokens]\n",
    "    # Create a regex pattern to match special tokens\n",
    "    special_tokens_pattern = '|'.join(escaped_tokens)\n",
    "    \n",
    "    # Regex pattern to split by special tokens or spaces\n",
    "    # Use capturing groups to preserve delimiters in the output\n",
    "    pattern = f'({special_tokens_pattern})|( )'\n",
    "    \n",
    "    # Split text using the pattern\n",
    "    tokens = re.split(pattern, text)\n",
    "    \n",
    "    # Filter out None values but keep empty strings (which represent spaces)\n",
    "    tokens = [token for token in tokens if token is not None]\n",
    "    \n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example input string with a special token and spaces\n",
    "input_text = \"<pad> hello world\"\n",
    "\n",
    "# Pre-tokenize the input string\n",
    "tokens = pre_tokenize(input_text, ['<pad>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '<pad>', '', ' ', 'hello', ' ', 'world']"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text: <pad> hello world\n",
      "Tokens: ['<pad>', ' ', 'h', 'e', 'l', 'lo', ' ', 'wo', 'rl', 'd']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Example special tokens\n",
    "special_tokens = ['<pad>', '[CLS]', '[SEP]', '[MASK]']\n",
    "\n",
    "# Example vocabulary and merges (for demonstration purposes)\n",
    "vocab = special_tokens + list('abcdefghijklmnopqrstuvwxyz')\n",
    "merges = {('l', 'o'): 'lo', ('w', 'o'): 'wo', ('lo', 'r'): 'lor', ('r', 'l'): 'rl'}\n",
    "\n",
    "def pre_tokenize(text, special_tokens):\n",
    "    # Escape special tokens for regex\n",
    "    escaped_tokens = [re.escape(token) for token in special_tokens]\n",
    "    # Create a regex pattern to match special tokens or spaces\n",
    "    special_tokens_pattern = '|'.join(escaped_tokens)\n",
    "    \n",
    "    # Split text using special tokens\n",
    "    tokens = re.split(f'({special_tokens_pattern})', text)\n",
    "    \n",
    "    # Further split by spaces but keep the special tokens intact\n",
    "    result = []\n",
    "    for token in tokens:\n",
    "        if token in special_tokens:\n",
    "            result.append(token)\n",
    "        else:\n",
    "            # Split by space and keep spaces as separate tokens\n",
    "            sub_tokens = re.split(r'(\\s+)', token)\n",
    "            result.extend([sub_token for sub_token in sub_tokens if sub_token])\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    pre_tokenized_text = pre_tokenize(text, special_tokens)\n",
    "    \n",
    "    # Split tokens into characters, but preserve special tokens\n",
    "    splits = []\n",
    "    for word in pre_tokenized_text:\n",
    "        if word in special_tokens or word == ' ':\n",
    "            splits.append([word])\n",
    "        else:\n",
    "            splits.append([char for char in word])\n",
    "\n",
    "    # Apply BPE merges\n",
    "    for pair, merge in merges.items():\n",
    "        for idx, split in enumerate(splits):\n",
    "            if split[0] in special_tokens or split[0] == ' ':\n",
    "                continue\n",
    "            i = 0\n",
    "            while i < len(split) - 1:\n",
    "                if split[i] == pair[0] and split[i + 1] == pair[1]:\n",
    "                    split = split[:i] + [merge] + split[i + 2 :]\n",
    "                else:\n",
    "                    i += 1\n",
    "            splits[idx] = split\n",
    "\n",
    "    # Flatten the list of lists\n",
    "    return sum(splits, [])\n",
    "\n",
    "# Example usage\n",
    "input_text = \"<pad> hello world\"\n",
    "\n",
    "tokens = tokenize(input_text)\n",
    "\n",
    "print(\"Input text:\", input_text)\n",
    "print(\"Tokens:\", tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_tokenized_text = pre_tokenize(input_text, special_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '<pad>', '', ' ', 'hello', ' ', 'world']"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_tokenized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = []\n",
    "for word in pre_tokenized_text:\n",
    "    if word in special_tokens or word == ' ':\n",
    "        splits.append([word])\n",
    "    else:\n",
    "        splits.append([char for char in word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[],\n",
       " ['<pad>'],\n",
       " [],\n",
       " [' '],\n",
       " ['h', 'e', 'l', 'l', 'o'],\n",
       " [' '],\n",
       " ['w', 'o', 'r', 'l', 'd']]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizer\n",
    "\n",
    "# Define special tokens\n",
    "special_tokens_test = ['[CLS]', '[SEP]', '[PAD]', '[MASK]']\n",
    "\n",
    "# Define a small vocabulary for BPE tokenization\n",
    "vocab_test = {\n",
    "    '[CLS]': 0,\n",
    "    '[SEP]': 1,\n",
    "    '[PAD]': 2,\n",
    "    '[MASK]': 3,\n",
    "    'Hello': 4,\n",
    "    'world': 5,\n",
    "    ',': 6,\n",
    "    '!': 7,\n",
    "    'H': 8,\n",
    "    'e': 9,\n",
    "    'l': 10,\n",
    "    'o': 11,\n",
    "    'w': 12,\n",
    "    'r': 13,\n",
    "    'd': 14,\n",
    "    'lo': 15,\n",
    "    'ld': 16,\n",
    "    'Hell': 17,\n",
    "    'wor': 18,\n",
    "    'orl': 19,\n",
    "    'rl': 20\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text: <pad> hello world [CLS] test [SEP]\n",
      "Tokens: ['<pad>', ' ', 'hello', ' ', 'world', ' ', '[CLS]', ' ', 'test', ' ', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def pre_tokenize(text, special_tokens):\n",
    "    # Escape special tokens for regex\n",
    "    escaped_tokens = [re.escape(token) for token in special_tokens]\n",
    "    # Create a regex pattern to match special tokens\n",
    "    special_tokens_pattern = '|'.join(escaped_tokens)\n",
    "    \n",
    "    # Pattern to match special tokens, words, and spaces\n",
    "    pattern = f'({special_tokens_pattern})|(\\\\s+)|(\\\\S+)'\n",
    "    \n",
    "    # Split text using the pattern\n",
    "    tokens = re.findall(pattern, text)\n",
    "    \n",
    "    # Flatten the list of tuples returned by re.findall and filter out empty strings\n",
    "    tokens = [token for group in tokens for token in group if token]\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "# Example usage\n",
    "input_text = \"<pad> hello world [CLS] test [SEP]\"\n",
    "\n",
    "tokens = pre_tokenize(input_text, special_tokens)\n",
    "\n",
    "print(\"Input text:\", input_text)\n",
    "print(\"Tokens:\", tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text: <pad> hello world [CLS] test [SEP] hello world\n",
      "Tokens: ['<pad>', ' hello', ' world', ' [CLS]', ' test', ' [SEP]', ' hello', ' world']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def pre_tokenize(text, special_tokens):\n",
    "    # Escape special tokens for regex\n",
    "    escaped_tokens = [re.escape(token) for token in special_tokens]\n",
    "    # Create a regex pattern to match special tokens\n",
    "    special_tokens_pattern = '|'.join(escaped_tokens)\n",
    "    \n",
    "    # Pattern to match special tokens or sequences of non-whitespace characters preceded by optional spaces\n",
    "    pattern = f'({special_tokens_pattern})|(\\\\s*\\\\S+)'\n",
    "    \n",
    "    # Split text using the pattern\n",
    "    tokens = re.findall(pattern, text)\n",
    "    \n",
    "    # Flatten the list of tuples returned by re.findall and filter out empty strings\n",
    "    tokens = [token for group in tokens for token in group if token]\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "# Example usage\n",
    "input_text = \"<pad> hello world [CLS] test [SEP] hello world\"\n",
    "\n",
    "tokens = pre_tokenize(input_text, special_tokens)\n",
    "\n",
    "print(\"Input text:\", input_text)\n",
    "print(\"Tokens:\", tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/bobby/code-repo/astar-projects/project-smallville/data/input.txt', 'r') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "class character:\n",
    "    def __init__(self):\n",
    "        with open('/home/bobby/code-repo/astar-projects/project-smallville/data/input.txt', 'r') as f:\n",
    "            self.text = f.read()\n",
    "        self.char = sorted(list(set(self.text)))\n",
    "        self.vocab = {ch:i for i,ch in enumerate(self.char)} ## char to id\n",
    "        # self.vocab_size = len(self.vocab)\n",
    "\n",
    "        self.special_tokens = [\n",
    "            'pad_token',\n",
    "            'mask_token'\n",
    "        ]\n",
    "        self.pad_token = None\n",
    "        self.pad_token_id = None\n",
    "        self.mask_token = None\n",
    "        self.mask_token_id = None\n",
    "    \n",
    "    def add_special_tokens(self, dict):\n",
    "        '''\n",
    "        This function will add special tokens to the vocabulary.\n",
    "        '''\n",
    "        assert set(dict.keys()).issubset(self.special_tokens), 'Invalid special tokens'\n",
    "\n",
    "        ## get the current vocab size at the current point in time\n",
    "        current_vocab_size = len(self.vocab)\n",
    "\n",
    "        ## add the special tokens to the vocab\n",
    "        for token in dict.keys():\n",
    "            self.vocab[dict[token]] = current_vocab_size\n",
    "            current_vocab_size += 1\n",
    "        \n",
    "        ## update the special tokens\n",
    "        self.pad_token = dict['pad_token']\n",
    "        self.pad_token_id = self.vocab[self.pad_token]\n",
    "        self.mask_token = dict['mask_token']\n",
    "        self.mask_token_id = self.vocab[self.mask_token]\n",
    "\n",
    "    @property\n",
    "    def vocab_size(self):\n",
    "        return len(self.vocab)\n",
    "\n",
    "    def decode(self, ids):\n",
    "        '''\n",
    "        Given a list of ids, this function will return the decoded text.\n",
    "        '''\n",
    "        idx_to_char = {i:ch for ch,i in self.vocab.items()}\n",
    "        text = ''.join([idx_to_char[i] for i in ids])\n",
    "        return text        \n",
    "\n",
    "\n",
    "    def encode(self, text):\n",
    "        '''\n",
    "        Given a text, this function will return the encoded ids.\n",
    "        '''\n",
    "        tokens = []\n",
    "        # words = text.split()\n",
    "        # for word in words:\n",
    "        #     if word in [self.pad_token, self.mask_token]:\n",
    "        #         tokens.append(word)\n",
    "        #     else:\n",
    "        #         tokens.extend(list(word))\n",
    "\n",
    "        i = 0\n",
    "\n",
    "        while i < len(text):\n",
    "            # Check if the current substring matches any special token\n",
    "            matched_special_token = False\n",
    "            for special_token in [self.pad_token, self.mask_token]:\n",
    "                if text[i:i+len(special_token)] == special_token:\n",
    "                    tokens.append(special_token)\n",
    "                    i += len(special_token)\n",
    "                    matched_special_token = True\n",
    "                    break\n",
    "\n",
    "            if not matched_special_token:\n",
    "                # If not a special token, tokenize character by character\n",
    "                tokens.append(text[i])\n",
    "                i += 1\n",
    "                \n",
    "        out = [self.vocab[ch] for ch in tokens]\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = character()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[154], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mdecode(tokenizer\u001b[38;5;241m.\u001b[39mencode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeef defef\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "Cell \u001b[0;32mIn[151], line 69\u001b[0m, in \u001b[0;36mcharacter.encode\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     67\u001b[0m matched_special_token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m special_token \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpad_token, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmask_token]:\n\u001b[0;32m---> 69\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m text[i:i\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mlen\u001b[39m(special_token)] \u001b[38;5;241m==\u001b[39m special_token:\n\u001b[1;32m     70\u001b[0m         tokens\u001b[38;5;241m.\u001b[39mappend(special_token)\n\u001b[1;32m     71\u001b[0m         i \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(special_token)\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer.encode('feef defef'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.add_special_tokens({'pad_token':'<pad>', 'mask_token':'<mask>'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.mask_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'\\n': 0,\n",
       " ' ': 1,\n",
       " '!': 2,\n",
       " '$': 3,\n",
       " '&': 4,\n",
       " \"'\": 5,\n",
       " ',': 6,\n",
       " '-': 7,\n",
       " '.': 8,\n",
       " '3': 9,\n",
       " ':': 10,\n",
       " ';': 11,\n",
       " '?': 12,\n",
       " 'A': 13,\n",
       " 'B': 14,\n",
       " 'C': 15,\n",
       " 'D': 16,\n",
       " 'E': 17,\n",
       " 'F': 18,\n",
       " 'G': 19,\n",
       " 'H': 20,\n",
       " 'I': 21,\n",
       " 'J': 22,\n",
       " 'K': 23,\n",
       " 'L': 24,\n",
       " 'M': 25,\n",
       " 'N': 26,\n",
       " 'O': 27,\n",
       " 'P': 28,\n",
       " 'Q': 29,\n",
       " 'R': 30,\n",
       " 'S': 31,\n",
       " 'T': 32,\n",
       " 'U': 33,\n",
       " 'V': 34,\n",
       " 'W': 35,\n",
       " 'X': 36,\n",
       " 'Y': 37,\n",
       " 'Z': 38,\n",
       " 'a': 39,\n",
       " 'b': 40,\n",
       " 'c': 41,\n",
       " 'd': 42,\n",
       " 'e': 43,\n",
       " 'f': 44,\n",
       " 'g': 45,\n",
       " 'h': 46,\n",
       " 'i': 47,\n",
       " 'j': 48,\n",
       " 'k': 49,\n",
       " 'l': 50,\n",
       " 'm': 51,\n",
       " 'n': 52,\n",
       " 'o': 53,\n",
       " 'p': 54,\n",
       " 'q': 55,\n",
       " 'r': 56,\n",
       " 's': 57,\n",
       " 't': 58,\n",
       " 'u': 59,\n",
       " 'v': 60,\n",
       " 'w': 61,\n",
       " 'x': 62,\n",
       " 'y': 63,\n",
       " 'z': 64,\n",
       " '<pad>': 65,\n",
       " '<mask>': 66}"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ewfeef <pad>'"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer.encode('ewfeef <pad>'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'<'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[150], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mdecode(tokenizer\u001b[38;5;241m.\u001b[39mencode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeef <pad>\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "Cell \u001b[0;32mIn[141], line 80\u001b[0m, in \u001b[0;36mcharacter.encode\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     77\u001b[0m         tokens\u001b[38;5;241m.\u001b[39mappend(text[i])\n\u001b[1;32m     78\u001b[0m         i \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 80\u001b[0m out \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab[ch] \u001b[38;5;28;01mfor\u001b[39;00m ch \u001b[38;5;129;01min\u001b[39;00m tokens]\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "Cell \u001b[0;32mIn[141], line 80\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     77\u001b[0m         tokens\u001b[38;5;241m.\u001b[39mappend(text[i])\n\u001b[1;32m     78\u001b[0m         i \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 80\u001b[0m out \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab[ch] \u001b[38;5;28;01mfor\u001b[39;00m ch \u001b[38;5;129;01min\u001b[39;00m tokens]\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[0;31mKeyError\u001b[0m: '<'"
     ]
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer.encode('feef <pad>'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTokenizer:\n",
    "    def __init__(self):\n",
    "        self.vocab = {\n",
    "            \"hello\": 0,\n",
    "            \"world\": 1,\n",
    "            \"I\": 2,\n",
    "            \"am\": 3,\n",
    "            \"a\": 4,\n",
    "            \"tokenizer\": 5\n",
    "        }\n",
    "        self.inv_vocab = {v: k for k, v in self.vocab.items()}\n",
    "        self.pad_token = None\n",
    "        self.mask_token = None\n",
    "        self.pad_token_id = None\n",
    "        self.mask_token_id = None\n",
    "        self.vocab_len = len(self.vocab)\n",
    "\n",
    "    def add_special_tokens(self, pad_token, mask_token):\n",
    "        # Add special tokens to the vocabulary\n",
    "        original_vocab_size = len(self.vocab)\n",
    "        self.vocab[pad_token] = original_vocab_size\n",
    "        self.vocab[mask_token] = original_vocab_size + 1\n",
    "\n",
    "        # Update inverse vocabulary\n",
    "        self.inv_vocab[original_vocab_size] = pad_token\n",
    "        self.inv_vocab[original_vocab_size + 1] = mask_token\n",
    "\n",
    "        # Store special tokens and their IDs\n",
    "        self.pad_token = pad_token\n",
    "        self.mask_token = mask_token\n",
    "        self.pad_token_id = self.vocab[pad_token]\n",
    "        self.mask_token_id = self.vocab[mask_token]\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        # Use updated vocabulary to tokenize text\n",
    "        return [self.vocab.get(token) for token in text.split()]\n",
    "\n",
    "    def convert_ids_to_tokens(self, ids):\n",
    "        return [self.inv_vocab.get(id, \"<unk>\") for id in ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tokenizer = CustomTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hello': 0, 'world': 1, 'I': 2, 'am': 3, 'a': 4, 'tokenizer': 5}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_tokenizer.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, None, 1]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_tokenizer.tokenize('hello <pad> world')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_tokenizer.vocab_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tokenizer.add_special_tokens('<pad>', '<mask>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 6, 1]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_tokenizer.tokenize('hello <pad> world')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_tokenizer.vocab_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_tokenizer.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'T'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[57], line 42\u001b[0m\n\u001b[1;32m     38\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis is a BPE tokenizer example\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Step 4: Output\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# Encode text\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m encoded_text \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEncoded Text:\u001b[39m\u001b[38;5;124m\"\u001b[39m, encoded_text)\n",
      "Cell \u001b[0;32mIn[57], line 10\u001b[0m, in \u001b[0;36mBPETokenizer.tokenize\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m      7\u001b[0m subwords \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bpe_tokenization(text)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Step 3: Subword Units to Token IDs\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m token_ids \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab[subword] \u001b[38;5;28;01mfor\u001b[39;00m subword \u001b[38;5;129;01min\u001b[39;00m subwords]\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m token_ids\n",
      "Cell \u001b[0;32mIn[57], line 10\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      7\u001b[0m subwords \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bpe_tokenization(text)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Step 3: Subword Units to Token IDs\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m token_ids \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab[subword] \u001b[38;5;28;01mfor\u001b[39;00m subword \u001b[38;5;129;01min\u001b[39;00m subwords]\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m token_ids\n",
      "\u001b[0;31mKeyError\u001b[0m: 'T'"
     ]
    }
   ],
   "source": [
    "class BPETokenizer:\n",
    "    def __init__(self, vocab):\n",
    "        self.vocab = vocab\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        # Step 2: Tokenization\n",
    "        subwords = self._bpe_tokenization(text)\n",
    "\n",
    "        # Step 3: Subword Units to Token IDs\n",
    "        token_ids = [self.vocab[subword] for subword in subwords]\n",
    "\n",
    "        return token_ids\n",
    "\n",
    "    def _bpe_tokenization(self, text):\n",
    "        # Placeholder BPE tokenization method\n",
    "        # Replace this with your BPE tokenization logic\n",
    "        # For demonstration, let's split words into characters\n",
    "        return list(text)\n",
    "\n",
    "# Step 1: Initialize Vocabulary\n",
    "# Example vocabulary mapping subword units to token IDs\n",
    "vocab = {\n",
    "    \"This\": 0,\n",
    "    \"is\": 1,\n",
    "    \"a\": 2,\n",
    "    \"B\": 3,\n",
    "    \"P\": 4,\n",
    "    \"E\": 5,\n",
    "    \"token\": 6,\n",
    "    \"izer\": 7,\n",
    "    \"example\": 8\n",
    "}\n",
    "\n",
    "# Initialize BPE tokenizer\n",
    "tokenizer = BPETokenizer(vocab)\n",
    "\n",
    "# Input text\n",
    "text = \"This is a BPE tokenizer example\"\n",
    "\n",
    "# Step 4: Output\n",
    "# Encode text\n",
    "encoded_text = tokenizer.tokenize(text)\n",
    "print(\"Encoded Text:\", encoded_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['T',\n",
       " 'h',\n",
       " 'i',\n",
       " 's',\n",
       " ' ',\n",
       " 'i',\n",
       " 's',\n",
       " ' ',\n",
       " 'a',\n",
       " ' ',\n",
       " 'B',\n",
       " 'P',\n",
       " 'E',\n",
       " ' ',\n",
       " 't',\n",
       " 'o',\n",
       " 'k',\n",
       " 'e',\n",
       " 'n',\n",
       " 'i',\n",
       " 'z',\n",
       " 'e',\n",
       " 'r',\n",
       " ' ',\n",
       " 'e',\n",
       " 'x',\n",
       " 'a',\n",
       " 'm',\n",
       " 'p',\n",
       " 'l',\n",
       " 'e']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(\"This is a BPE tokenizer example\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = character()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'feef'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer.encode('feef'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab = {ch:i for i,ch in enumerate(chars)} ## char to id\n",
    "idx_to_char = {i:ch for ch,i in vocab.items()} ## id to char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'\\n': 0,\n",
       " ' ': 1,\n",
       " '!': 2,\n",
       " '$': 3,\n",
       " '&': 4,\n",
       " \"'\": 5,\n",
       " ',': 6,\n",
       " '-': 7,\n",
       " '.': 8,\n",
       " '3': 9,\n",
       " ':': 10,\n",
       " ';': 11,\n",
       " '?': 12,\n",
       " 'A': 13,\n",
       " 'B': 14,\n",
       " 'C': 15,\n",
       " 'D': 16,\n",
       " 'E': 17,\n",
       " 'F': 18,\n",
       " 'G': 19,\n",
       " 'H': 20,\n",
       " 'I': 21,\n",
       " 'J': 22,\n",
       " 'K': 23,\n",
       " 'L': 24,\n",
       " 'M': 25,\n",
       " 'N': 26,\n",
       " 'O': 27,\n",
       " 'P': 28,\n",
       " 'Q': 29,\n",
       " 'R': 30,\n",
       " 'S': 31,\n",
       " 'T': 32,\n",
       " 'U': 33,\n",
       " 'V': 34,\n",
       " 'W': 35,\n",
       " 'X': 36,\n",
       " 'Y': 37,\n",
       " 'Z': 38,\n",
       " 'a': 39,\n",
       " 'b': 40,\n",
       " 'c': 41,\n",
       " 'd': 42,\n",
       " 'e': 43,\n",
       " 'f': 44,\n",
       " 'g': 45,\n",
       " 'h': 46,\n",
       " 'i': 47,\n",
       " 'j': 48,\n",
       " 'k': 49,\n",
       " 'l': 50,\n",
       " 'm': 51,\n",
       " 'n': 52,\n",
       " 'o': 53,\n",
       " 'p': 54,\n",
       " 'q': 55,\n",
       " 'r': 56,\n",
       " 's': 57,\n",
       " 't': 58,\n",
       " 'u': 59,\n",
       " 'v': 60,\n",
       " 'w': 61,\n",
       " 'x': 62,\n",
       " 'y': 63,\n",
       " 'z': 64}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '\\n',\n",
       " 1: ' ',\n",
       " 2: '!',\n",
       " 3: '$',\n",
       " 4: '&',\n",
       " 5: \"'\",\n",
       " 6: ',',\n",
       " 7: '-',\n",
       " 8: '.',\n",
       " 9: '3',\n",
       " 10: ':',\n",
       " 11: ';',\n",
       " 12: '?',\n",
       " 13: 'A',\n",
       " 14: 'B',\n",
       " 15: 'C',\n",
       " 16: 'D',\n",
       " 17: 'E',\n",
       " 18: 'F',\n",
       " 19: 'G',\n",
       " 20: 'H',\n",
       " 21: 'I',\n",
       " 22: 'J',\n",
       " 23: 'K',\n",
       " 24: 'L',\n",
       " 25: 'M',\n",
       " 26: 'N',\n",
       " 27: 'O',\n",
       " 28: 'P',\n",
       " 29: 'Q',\n",
       " 30: 'R',\n",
       " 31: 'S',\n",
       " 32: 'T',\n",
       " 33: 'U',\n",
       " 34: 'V',\n",
       " 35: 'W',\n",
       " 36: 'X',\n",
       " 37: 'Y',\n",
       " 38: 'Z',\n",
       " 39: 'a',\n",
       " 40: 'b',\n",
       " 41: 'c',\n",
       " 42: 'd',\n",
       " 43: 'e',\n",
       " 44: 'f',\n",
       " 45: 'g',\n",
       " 46: 'h',\n",
       " 47: 'i',\n",
       " 48: 'j',\n",
       " 49: 'k',\n",
       " 50: 'l',\n",
       " 51: 'm',\n",
       " 52: 'n',\n",
       " 53: 'o',\n",
       " 54: 'p',\n",
       " 55: 'q',\n",
       " 56: 'r',\n",
       " 57: 's',\n",
       " 58: 't',\n",
       " 59: 'u',\n",
       " 60: 'v',\n",
       " 61: 'w',\n",
       " 62: 'x',\n",
       " 63: 'y',\n",
       " 64: 'z'}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_to_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AddedToken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AddedToken(\"<PAD>\", rstrip=True, lstrip=False, single_word=False, normalized=True, special=False)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check = AddedToken('<PAD>', lstrip=False, rstrip=True)\n",
    "check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tokenizers.AddedToken' object has no attribute 'id'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m check\u001b[38;5;241m.\u001b[39mid\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tokenizers.AddedToken' object has no attribute 'id'"
     ]
    }
   ],
   "source": [
    "check.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "char = sorted(list(set(text)))\n",
    "\n",
    "char.append('<pad>')\n",
    "char.append('<mask>')\n",
    "\n",
    "vocab_size = len(char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '\\n',\n",
       " 1: ' ',\n",
       " 2: '!',\n",
       " 3: '$',\n",
       " 4: '&',\n",
       " 5: \"'\",\n",
       " 6: ',',\n",
       " 7: '-',\n",
       " 8: '.',\n",
       " 9: '3',\n",
       " 10: ':',\n",
       " 11: ';',\n",
       " 12: '?',\n",
       " 13: 'A',\n",
       " 14: 'B',\n",
       " 15: 'C',\n",
       " 16: 'D',\n",
       " 17: 'E',\n",
       " 18: 'F',\n",
       " 19: 'G',\n",
       " 20: 'H',\n",
       " 21: 'I',\n",
       " 22: 'J',\n",
       " 23: 'K',\n",
       " 24: 'L',\n",
       " 25: 'M',\n",
       " 26: 'N',\n",
       " 27: 'O',\n",
       " 28: 'P',\n",
       " 29: 'Q',\n",
       " 30: 'R',\n",
       " 31: 'S',\n",
       " 32: 'T',\n",
       " 33: 'U',\n",
       " 34: 'V',\n",
       " 35: 'W',\n",
       " 36: 'X',\n",
       " 37: 'Y',\n",
       " 38: 'Z',\n",
       " 39: 'a',\n",
       " 40: 'b',\n",
       " 41: 'c',\n",
       " 42: 'd',\n",
       " 43: 'e',\n",
       " 44: 'f',\n",
       " 45: 'g',\n",
       " 46: 'h',\n",
       " 47: 'i',\n",
       " 48: 'j',\n",
       " 49: 'k',\n",
       " 50: 'l',\n",
       " 51: 'm',\n",
       " 52: 'n',\n",
       " 53: 'o',\n",
       " 54: 'p',\n",
       " 55: 'q',\n",
       " 56: 'r',\n",
       " 57: 's',\n",
       " 58: 't',\n",
       " 59: 'u',\n",
       " 60: 'v',\n",
       " 61: 'w',\n",
       " 62: 'x',\n",
       " 63: 'y',\n",
       " 64: 'z',\n",
       " 65: '<pad>',\n",
       " 66: '<mask>'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_to_char = {i:ch for i, ch in enumerate(char)}\n",
    "idx_to_char"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "smallville",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
