{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1996, 3609, 1997, 1996, 3712, 2003, 103, 1012]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased',\n",
    "                                          cache_dir='/data/bobby/huggingface-cache/models')\n",
    "model = BertForMaskedLM.from_pretrained('bert-base-uncased',\n",
    "                                        cache_dir='/data/bobby/huggingface-cache/models')\n",
    "\n",
    "text = \"The color of the sky is [MASK].\"\n",
    "\n",
    "tokens = tokenizer.tokenize(text)\n",
    "masked_index = tokens.index('[MASK]')\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "token_ids[masked_index] = tokenizer.mask_token_id\n",
    "input_tensor = torch.tensor([token_ids])\n",
    "\n",
    "print(token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 30522])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1012"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(outputs[0][0, masked_index]).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'.'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(torch.argmax(outputs[0][0, masked_index]).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = outputs[0]\n",
    "predicted_index = torch.argmax(predictions[0, masked_index]).item()\n",
    "predicted_token = tokenizer.convert_ids_to_tokens([predicted_index])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'.'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[UNK]'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(-33434343434)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([68, 71, 61, 43, 66])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.randint(100, (5,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 0, 1, 1, 0, 1, 0, 0, 1])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randint(0, 2, (10,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Channels:\n",
      " - defaults\n",
      " - conda-forge\n",
      " - nvidia\n",
      " - pytorch\n",
      "Platform: linux-64\n",
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /home/bobby/miniconda3/envs/smallville\n",
      "\n",
      "  added / updated specs:\n",
      "    - conda-forge::transformers\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    datasets-2.19.1            |     pyhd8ed1ab_0         350 KB  conda-forge\n",
      "    huggingface_hub-0.23.0     |     pyhd8ed1ab_0         244 KB  conda-forge\n",
      "    multiprocess-0.70.15       |  py311h06a4308_0         342 KB\n",
      "    pyarrow-hotfix-0.6         |     pyhd8ed1ab_0          13 KB  conda-forge\n",
      "    python-xxhash-2.0.2        |  py311h5eee18b_1          20 KB\n",
      "    safetensors-0.4.2          |  py311h24d97f6_0         1.1 MB\n",
      "    tokenizers-0.19.1          |  py311h6640629_0         2.6 MB  conda-forge\n",
      "    transformers-4.40.2        |     pyhd8ed1ab_0         3.1 MB  conda-forge\n",
      "    xxhash-0.8.0               |       h7f8727e_3          83 KB\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:         7.8 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  datasets           conda-forge/noarch::datasets-2.19.1-pyhd8ed1ab_0 \n",
      "  huggingface_hub    conda-forge/noarch::huggingface_hub-0.23.0-pyhd8ed1ab_0 \n",
      "  multiprocess       pkgs/main/linux-64::multiprocess-0.70.15-py311h06a4308_0 \n",
      "  pyarrow-hotfix     conda-forge/noarch::pyarrow-hotfix-0.6-pyhd8ed1ab_0 \n",
      "  python-xxhash      pkgs/main/linux-64::python-xxhash-2.0.2-py311h5eee18b_1 \n",
      "  safetensors        pkgs/main/linux-64::safetensors-0.4.2-py311h24d97f6_0 \n",
      "  tokenizers         conda-forge/linux-64::tokenizers-0.19.1-py311h6640629_0 \n",
      "  transformers       conda-forge/noarch::transformers-4.40.2-pyhd8ed1ab_0 \n",
      "  xxhash             pkgs/main/linux-64::xxhash-0.8.0-h7f8727e_3 \n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages:\n",
      "transformers-4.40.2  | 3.1 MB    |                                       |   0% \n",
      "tokenizers-0.19.1    | 2.6 MB    |                                       |   0% \u001b[A\n",
      "\n",
      "safetensors-0.4.2    | 1.1 MB    |                                       |   0% \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "datasets-2.19.1      | 350 KB    |                                       |   0% \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "multiprocess-0.70.15 | 342 KB    |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "huggingface_hub-0.23 | 244 KB    |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "xxhash-0.8.0         | 83 KB     |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "python-xxhash-2.0.2  | 20 KB     |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "pyarrow-hotfix-0.6   | 13 KB     |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "multiprocess-0.70.15 | 342 KB    | #7                                    |   5% \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "safetensors-0.4.2    | 1.1 MB    | 5                                     |   1% \u001b[A\u001b[A\n",
      "transformers-4.40.2  | 3.1 MB    | 1                                     |   1% \u001b[A\n",
      "\n",
      "\n",
      "datasets-2.19.1      | 350 KB    | #6                                    |   5% \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "multiprocess-0.70.15 | 342 KB    | #############8                        |  37% \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "transformers-4.40.2  | 3.1 MB    | #3                                    |   4% \u001b[A\u001b[A\n",
      "tokenizers-0.19.1    | 2.6 MB    | #7                                    |   5% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "multiprocess-0.70.15 | 342 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "datasets-2.19.1      | 350 KB    | #############5                        |  37% \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "transformers-4.40.2  | 3.1 MB    | #####2                                |  14% \u001b[A\u001b[A\n",
      "tokenizers-0.19.1    | 2.6 MB    | #####2                                |  14% \u001b[A\n",
      "\n",
      "safetensors-0.4.2    | 1.1 MB    | ##################################### | 100% \u001b[A\u001b[A\n",
      "\n",
      "safetensors-0.4.2    | 1.1 MB    | ##################################### | 100% \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "datasets-2.19.1      | 350 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "transformers-4.40.2  | 3.1 MB    | ###############2                      |  41% \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "python-xxhash-2.0.2  | 20 KB     | #############################3        |  79% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "tokenizers-0.19.1    | 2.6 MB    | #############9                        |  38% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "python-xxhash-2.0.2  | 20 KB     | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "tokenizers-0.19.1    | 2.6 MB    | ####################################8 | 100% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "huggingface_hub-0.23 | 244 KB    | ##4                                   |   7% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "xxhash-0.8.0         | 83 KB     | #######1                              |  19% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "tokenizers-0.19.1    | 2.6 MB    | ##################################### | 100% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "xxhash-0.8.0         | 83 KB     | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "huggingface_hub-0.23 | 244 KB    | ###################3                  |  52% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "huggingface_hub-0.23 | 244 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "pyarrow-hotfix-0.6   | 13 KB     | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "conda install conda-forge::transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bobby/miniconda3/envs/smallville/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "import numpy as np\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, max_len=128, mlm_prob=0.15):\n",
    "        self.texts = texts\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.mlm_prob = mlm_prob\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        tokens = self.tokenizer.encode(text, add_special_tokens=True, max_length=self.max_len, truncation=True)\n",
    "        \n",
    "        input_ids = tokens\n",
    "        attention_mask = [1] * len(input_ids)\n",
    "        labels = input_ids.copy()\n",
    "\n",
    "        for i in range(len(input_ids)):\n",
    "            if random.random() < self.mlm_prob:\n",
    "                input_ids[i] = self.tokenizer.mask_token_id\n",
    "\n",
    "        padding_length = self.max_len - len(input_ids)\n",
    "        input_ids = input_ids + ([self.tokenizer.pad_token_id] * padding_length)\n",
    "        attention_mask = attention_mask + ([0] * padding_length)\n",
    "        labels = labels + ([-100] * padding_length)\n",
    "\n",
    "        return torch.tensor(input_ids), torch.tensor(attention_mask), torch.tensor(labels)\n",
    "\n",
    "# Example usage\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.add_special_tokens({'mask_token': '[MASK]', 'pad_token': '[PAD]'})\n",
    "texts = [\"There is a cow with black and white spots which I believe is the cousin of a zebra.\", \"I think it is amazing to see Roger Federer at his peak once in my lifetime.\"]  # Replace with your dataset\n",
    "dataset = TextDataset(texts, tokenizer)\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = dataset.texts[0]\n",
    "tokens = dataset.tokenizer.encode(text, add_special_tokens=True, max_length=128, truncation=True)\n",
    "\n",
    "input_ids = tokens\n",
    "attention_mask = [1] * len(input_ids)\n",
    "labels = input_ids.copy()\n",
    "\n",
    "for i in range(len(input_ids)):\n",
    "    if random.random() < 0.15:\n",
    "        input_ids[i] = dataset.tokenizer.mask_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1858,\n",
       " 318,\n",
       " 257,\n",
       " 9875,\n",
       " 50257,\n",
       " 2042,\n",
       " 290,\n",
       " 2330,\n",
       " 10222,\n",
       " 543,\n",
       " 314,\n",
       " 1975,\n",
       " 318,\n",
       " 262,\n",
       " 16933,\n",
       " 286,\n",
       " 257,\n",
       " 1976,\n",
       " 37052,\n",
       " 13]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "padding_length = dataset.max_len - len(input_ids)\n",
    "input_ids = input_ids + ([dataset.tokenizer.pad_token_id] * padding_length)\n",
    "attention_mask = attention_mask + ([0] * padding_length)\n",
    "labels = labels + ([-100] * padding_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1858,\n",
       " 318,\n",
       " 257,\n",
       " 9875,\n",
       " 50257,\n",
       " 2042,\n",
       " 290,\n",
       " 2330,\n",
       " 10222,\n",
       " 543,\n",
       " 314,\n",
       " 1975,\n",
       " 318,\n",
       " 262,\n",
       " 16933,\n",
       " 286,\n",
       " 257,\n",
       " 1976,\n",
       " 37052,\n",
       " 13,\n",
       " 50258,\n",
       " 50258,\n",
       " 50258,\n",
       " 50258,\n",
       " 50258,\n",
       " 50258,\n",
       " 50258,\n",
       " 50258,\n",
       " 50258,\n",
       " 50258,\n",
       " 50258,\n",
       " 50258,\n",
       " 50258,\n",
       " 50258,\n",
       " 50258,\n",
       " 50258,\n",
       " 50258,\n",
       " 50258,\n",
       " 50258,\n",
       " 50258,\n",
       " 50258,\n",
       " 50258,\n",
       " 50258,\n",
       " 50258,\n",
       " 50258,\n",
       " 50258,\n",
       " 50258,\n",
       " 50258,\n",
       " 50258,\n",
       " 50258,\n",
       " 50258,\n",
       " 50258,\n",
       " 50258,\n",
       " 50258,\n",
       " 50258,\n",
       " 50258,\n",
       " 50258,\n",
       " 50258,\n",
       " 50258,\n",
       " 50258,\n",
       " 50258,\n",
       " 50258,\n",
       " 50258,\n",
       " 50258,\n",
       " 50258,\n",
       " 50258,\n",
       " 50258,\n",
       " 50258,\n",
       " 50258,\n",
       " 50258,\n",
       " 50258,\n",
       " 50258,\n",
       " 50258,\n",
       " 50258,\n",
       " 50258,\n",
       " 50258,\n",
       " 50258,\n",
       " 50258,\n",
       " 50258,\n",
       " 50258,\n",
       " 50258,\n",
       " 50258,\n",
       " 50258,\n",
       " 50258,\n",
       " 50258,\n",
       " 50258,\n",
       " 50258,\n",
       " 50258,\n",
       " 50258,\n",
       " 50258,\n",
       " 50258,\n",
       " 50258,\n",
       " 50258,\n",
       " 50258,\n",
       " 50258,\n",
       " 50258,\n",
       " 50258,\n",
       " 50258,\n",
       " 50258,\n",
       " 50258,\n",
       " 50258,\n",
       " 50258,\n",
       " 50258,\n",
       " 50258,\n",
       " 50258,\n",
       " 50258,\n",
       " 50258,\n",
       " 50258,\n",
       " 50258,\n",
       " 50258,\n",
       " 50258,\n",
       " 50258,\n",
       " 50258,\n",
       " 50258,\n",
       " 50258,\n",
       " 50258,\n",
       " 50258,\n",
       " 50258,\n",
       " 50258,\n",
       " 50258,\n",
       " 50258,\n",
       " 50258,\n",
       " 50258,\n",
       " 50258,\n",
       " 50258,\n",
       " 50258,\n",
       " 50258,\n",
       " 50258]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 1858, 50257,   257,  9875,   351,  2042,   290,  2330, 10222, 50257,\n",
       "           314, 50257,   318, 50257, 50257,   286, 50257,  1976, 37052,    13,\n",
       "         50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258,\n",
       "         50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258,\n",
       "         50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258,\n",
       "         50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258,\n",
       "         50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258,\n",
       "         50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258,\n",
       "         50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258,\n",
       "         50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258,\n",
       "         50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258,\n",
       "         50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258,\n",
       "         50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258]),\n",
       " tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " tensor([ 1858,   318,   257,  9875,   351,  2042,   290,  2330, 10222,   543,\n",
       "           314,  1975,   318,   262, 16933,   286,   257,  1976, 37052,    13,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100]))"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 128]) torch.Size([2, 128]) torch.Size([2, 128])\n"
     ]
    }
   ],
   "source": [
    "for input_ids, attention_mask, labels in dataloader:\n",
    "    print(input_ids.shape, attention_mask.shape, labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50259"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset.tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.tokenizer.add_special_tokens({'mask_token': '[MASK]'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50257"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.tokenizer.convert_tokens_to_ids(dataset.tokenizer.mask_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Could not infer dtype of NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[58], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m dataset\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;241m0\u001b[39m)\n",
      "Cell \u001b[0;32mIn[57], line 34\u001b[0m, in \u001b[0;36mTextDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     31\u001b[0m attention_mask \u001b[38;5;241m=\u001b[39m attention_mask \u001b[38;5;241m+\u001b[39m ([\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m padding_length)\n\u001b[1;32m     32\u001b[0m labels \u001b[38;5;241m=\u001b[39m labels \u001b[38;5;241m+\u001b[39m ([\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100\u001b[39m] \u001b[38;5;241m*\u001b[39m padding_length)\n\u001b[0;32m---> 34\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mtensor(input_ids), torch\u001b[38;5;241m.\u001b[39mtensor(attention_mask), torch\u001b[38;5;241m.\u001b[39mtensor(labels)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Could not infer dtype of NoneType"
     ]
    }
   ],
   "source": [
    "dataset.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bobby/miniconda3/envs/smallville/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mask token ID: 50257\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "# Load the GPT2 tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "# Add a mask token to the tokenizer\n",
    "tokenizer.add_special_tokens({'mask_token': '[MASK]'})\n",
    "\n",
    "# Verify the mask token ID\n",
    "mask_token_id = tokenizer.convert_tokens_to_ids(tokenizer.mask_token)\n",
    "print(f'Mask token ID: {mask_token_id}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50257"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.mask_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 128])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 128])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[106], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m input_ids, attention_mask, labels \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[0;32m----> 2\u001b[0m     model(input_ids, attention_mask)\n",
      "File \u001b[0;32m~/miniconda3/envs/smallville/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/smallville/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[89], line 28\u001b[0m, in \u001b[0;36mGPT2Decoder.forward\u001b[0;34m(self, input_ids, attention_mask)\u001b[0m\n\u001b[1;32m     25\u001b[0m seq_len, batch_size \u001b[38;5;241m=\u001b[39m input_ids\u001b[38;5;241m.\u001b[39msize()\n\u001b[1;32m     26\u001b[0m pos_enc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpositional_encoding[:seq_len, :]\u001b[38;5;241m.\u001b[39mto(input_ids\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m---> 28\u001b[0m embedded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding(input_ids) \u001b[38;5;241m+\u001b[39m pos_enc\n\u001b[1;32m     29\u001b[0m embedded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(embedded)\n\u001b[1;32m     31\u001b[0m tgt_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_square_subsequent_mask(seq_len)\u001b[38;5;241m.\u001b[39mto(input_ids\u001b[38;5;241m.\u001b[39mdevice)\n",
      "File \u001b[0;32m~/miniconda3/envs/smallville/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/smallville/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/smallville/lib/python3.11/site-packages/torch/nn/modules/sparse.py:163\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 163\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39membedding(\n\u001b[1;32m    164\u001b[0m         \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_idx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_norm,\n\u001b[1;32m    165\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm_type, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale_grad_by_freq, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparse)\n",
      "File \u001b[0;32m~/miniconda3/envs/smallville/lib/python3.11/site-packages/torch/nn/functional.py:2264\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2258\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2259\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2260\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2261\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2262\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2263\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2264\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39membedding(weight, \u001b[38;5;28minput\u001b[39m, padding_idx, scale_grad_by_freq, sparse)\n",
      "\u001b[0;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "for input_ids, attention_mask, labels in dataloader:\n",
    "    model(input_ids, attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size, seq_len = input_ids.size()\n",
    "pos_enc = model.positional_encoding[:,:seq_len].to(input_ids.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 128])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 1, 768])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_enc.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 128])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 128, 768])"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.embedding(input_ids).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50258"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 768])"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_enc = torch.zeros(max_seq_len, d_model)\n",
    "pos_enc.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 1])"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position = torch.arange(0, max_seq_len, dtype=torch.float).unsqueeze(1)\n",
    "position.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([384])"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-torch.log(torch.tensor(10000.0)) / d_model))\n",
    "div_term.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 384])"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sin(position * div_term).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_enc[:, 0::2] = torch.sin(position * div_term)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_enc[:, 1::2] = torch.cos(position * div_term)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 768])"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_enc.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128, 768])"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_enc.unsqueeze(0).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = model.embedding(input_ids) + pos_enc.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 128, 768])"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 768])"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_enc.unsqueeze(0)[:, :, :].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_enc1 = model.positional_encoding[:, :seq_len, :]\n",
    "embedded1 = model.embedding(input_ids) + pos_enc1\n",
    "embedded1 = model.dropout(embedded1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 128])"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = (torch.triu(torch.ones(seq_len, seq_len)) == 1).transpose(0, 1)\n",
    "mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "mask.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
       "        [0., 0., -inf,  ..., -inf, -inf, -inf],\n",
       "        [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., -inf, -inf],\n",
       "        [0., 0., 0.,  ..., 0., 0., -inf],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_square_subsequent_mask(sz):\n",
    "    mask = torch.tril(torch.ones(sz, sz)) == 1\n",
    "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "    return mask\n",
    "\n",
    "tgt_mask = generate_square_subsequent_mask(input_ids.size(1))\n",
    "tgt_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 128])"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tgt_key_padding_mask = (input_ids == dataset.tokenizer.pad_token_id)\n",
    "tgt_key_padding_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True]])"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tgt_key_padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The expanded size of the tensor (128) must match the existing size (2) at non-singleton dimension 1.  Target sizes: [-1, 128, -1, -1].  Tensor sizes: [2, 1, 128]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[284], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m tgt_key_padding_mask_expanded \u001b[38;5;241m=\u001b[39m tgt_key_padding_mask\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# (batch_size, 1, 1, seq_len)\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m tgt_key_padding_mask_expanded \u001b[38;5;241m=\u001b[39m tgt_key_padding_mask_expanded\u001b[38;5;241m.\u001b[39mexpand(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, input_ids\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# (batch_size, seq_len, 1, seq_len)\u001b[39;00m\n\u001b[1;32m      3\u001b[0m tgt_key_padding_mask_expanded \u001b[38;5;241m=\u001b[39m tgt_key_padding_mask_expanded\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The expanded size of the tensor (128) must match the existing size (2) at non-singleton dimension 1.  Target sizes: [-1, 128, -1, -1].  Tensor sizes: [2, 1, 128]"
     ]
    }
   ],
   "source": [
    "tgt_key_padding_mask_expanded = tgt_key_padding_mask.unsqueeze(1)  # (batch_size, 1, 1, seq_len)\n",
    "tgt_key_padding_mask_expanded = tgt_key_padding_mask_expanded.expand(-1, input_ids.size(1), -1, -1)  # (batch_size, seq_len, 1, seq_len)\n",
    "tgt_key_padding_mask_expanded = tgt_key_padding_mask_expanded.transpose(1, 2)  # (batch_size, seq_len, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 128, 128])"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tgt_key_padding_mask_expanded.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False, False, False,  ...,  True,  True,  True],\n",
       "        [False, False, False,  ...,  True,  True,  True],\n",
       "        [False, False, False,  ...,  True,  True,  True],\n",
       "        ...,\n",
       "        [False, False, False,  ...,  True,  True,  True],\n",
       "        [False, False, False,  ...,  True,  True,  True],\n",
       "        [False, False, False,  ...,  True,  True,  True]])"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tgt_key_padding_mask_expanded[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 128])"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tgt_mask = generate_square_subsequent_mask(input_ids.size(1))\n",
    "tgt_mask.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = torch.randn(batch_size, seq_len, d_model)\n",
    "K = torch.randn(batch_size, seq_len, d_model)\n",
    "V = torch.randn(batch_size, seq_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 128, 128])"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_scores = Q @ K.transpose(-2, -1) / torch.sqrt(torch.tensor(d_model).float())\n",
    "attn_scores.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 128, 128])"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_scores = attn_scores.masked_fill(tgt_mask == float('-inf'), float('-inf'))\n",
    "attn_scores.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.4163,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
       "          [-0.3875,  0.7326,    -inf,  ...,    -inf,    -inf,    -inf],\n",
       "          [-1.3091, -0.5916, -1.1091,  ...,    -inf,    -inf,    -inf],\n",
       "          ...,\n",
       "          [ 0.9462, -2.0856,  0.2904,  ...,    -inf,    -inf,    -inf],\n",
       "          [-0.7204, -0.7541, -0.6241,  ...,    -inf,    -inf,    -inf],\n",
       "          [-0.6889, -0.0499, -1.3547,  ...,    -inf,    -inf,    -inf]],\n",
       "\n",
       "         [[-0.5757,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
       "          [ 1.9035,  0.8974,    -inf,  ...,    -inf,    -inf,    -inf],\n",
       "          [-0.6239, -0.7618, -0.1069,  ...,    -inf,    -inf,    -inf],\n",
       "          ...,\n",
       "          [ 0.7042,  1.2454, -1.8217,  ...,    -inf,    -inf,    -inf],\n",
       "          [ 0.8925,  0.5226,  0.1747,  ...,    -inf,    -inf,    -inf],\n",
       "          [-1.8551,  0.0817,  0.7889,  ...,    -inf,    -inf,    -inf]]],\n",
       "\n",
       "\n",
       "        [[[ 0.4163,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
       "          [-0.3875,  0.7326,    -inf,  ...,    -inf,    -inf,    -inf],\n",
       "          [-1.3091, -0.5916, -1.1091,  ...,    -inf,    -inf,    -inf],\n",
       "          ...,\n",
       "          [ 0.9462, -2.0856,  0.2904,  ...,    -inf,    -inf,    -inf],\n",
       "          [-0.7204, -0.7541, -0.6241,  ...,    -inf,    -inf,    -inf],\n",
       "          [-0.6889, -0.0499, -1.3547,  ...,    -inf,    -inf,    -inf]],\n",
       "\n",
       "         [[-0.5757,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
       "          [ 1.9035,  0.8974,    -inf,  ...,    -inf,    -inf,    -inf],\n",
       "          [-0.6239, -0.7618, -0.1069,  ...,    -inf,    -inf,    -inf],\n",
       "          ...,\n",
       "          [ 0.7042,  1.2454, -1.8217,  ...,    -inf,    -inf,    -inf],\n",
       "          [ 0.8925,  0.5226,  0.1747,  ...,    -inf,    -inf,    -inf],\n",
       "          [-1.8551,  0.0817,  0.7889,  ...,    -inf,    -inf,    -inf]]]])"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_scores.masked_fill(tgt_key_padding_mask_expanded == 1, float('-inf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import TransformerDecoder, TransformerDecoderLayer\n",
    "\n",
    "class GPT2Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, nhead, num_layers, dim_feedforward, max_seq_len):\n",
    "        super(GPT2Decoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.positional_encoding = self._generate_positional_encoding(max_seq_len, d_model)\n",
    "        self.decoder_layers = TransformerDecoderLayer(d_model, nhead, dim_feedforward)\n",
    "        self.transformer_decoder = TransformerDecoder(self.decoder_layers, num_layers)\n",
    "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def _generate_positional_encoding(self, max_seq_len, d_model):\n",
    "        pos_enc = torch.zeros(max_seq_len, d_model) # T, N\n",
    "        position = torch.arange(0, max_seq_len, dtype=torch.float).unsqueeze(1) # T, 1\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-torch.log(torch.tensor(10000.0)) / d_model)) # D/2\n",
    "        pos_enc[:, 0::2] = torch.sin(position * div_term) # T, D/2\n",
    "        pos_enc[:, 1::2] = torch.cos(position * div_term) # T, D/2\n",
    "        pos_enc = pos_enc.unsqueeze(0)#.transpose(0, 1) \n",
    "        return pos_enc\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        batch_size, seq_len = input_ids.size()\n",
    "        pos_enc = self.positional_encoding[:, :seq_len, :].to(input_ids.device)\n",
    "\n",
    "        embedded = self.embedding(input_ids) + pos_enc\n",
    "        embedded = self.dropout(embedded)\n",
    "\n",
    "        tgt_mask = self._generate_square_subsequent_mask(seq_len).to(input_ids.device)\n",
    "\n",
    "        memory = torch.zeros(seq_len, batch_size, embedded.size(-1)).to(input_ids.device)\n",
    "        output = self.transformer_decoder(embedded, memory, tgt_mask=tgt_mask, tgt_key_padding_mask=attention_mask)\n",
    "        output = self.fc_out(output)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def _generate_square_subsequent_mask(self, size):\n",
    "        mask = (torch.triu(torch.ones(size, size)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "only bool and floating types of key_padding_mask are supported",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[221], line 23\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m input_ids, attention_mask, labels \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[1;32m     22\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 23\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(input_ids, attention_mask)\n\u001b[1;32m     24\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_fn(outputs\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, vocab_size), labels\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     25\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/miniconda3/envs/smallville/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/smallville/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[220], line 34\u001b[0m, in \u001b[0;36mGPT2Decoder.forward\u001b[0;34m(self, input_ids, attention_mask)\u001b[0m\n\u001b[1;32m     31\u001b[0m tgt_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_square_subsequent_mask(seq_len)\u001b[38;5;241m.\u001b[39mto(input_ids\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     33\u001b[0m memory \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(seq_len, batch_size, embedded\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mto(input_ids\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m---> 34\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer_decoder(embedded, memory, tgt_mask\u001b[38;5;241m=\u001b[39mtgt_mask, tgt_key_padding_mask\u001b[38;5;241m=\u001b[39mattention_mask)\n\u001b[1;32m     35\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc_out(output)\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/miniconda3/envs/smallville/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/smallville/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/smallville/lib/python3.11/site-packages/torch/nn/modules/transformer.py:494\u001b[0m, in \u001b[0;36mTransformerDecoder.forward\u001b[0;34m(self, tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask, tgt_is_causal, memory_is_causal)\u001b[0m\n\u001b[1;32m    491\u001b[0m tgt_is_causal \u001b[38;5;241m=\u001b[39m _detect_is_causal_mask(tgt_mask, tgt_is_causal, seq_len)\n\u001b[1;32m    493\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m mod \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m--> 494\u001b[0m     output \u001b[38;5;241m=\u001b[39m mod(output, memory, tgt_mask\u001b[38;5;241m=\u001b[39mtgt_mask,\n\u001b[1;32m    495\u001b[0m                  memory_mask\u001b[38;5;241m=\u001b[39mmemory_mask,\n\u001b[1;32m    496\u001b[0m                  tgt_key_padding_mask\u001b[38;5;241m=\u001b[39mtgt_key_padding_mask,\n\u001b[1;32m    497\u001b[0m                  memory_key_padding_mask\u001b[38;5;241m=\u001b[39mmemory_key_padding_mask,\n\u001b[1;32m    498\u001b[0m                  tgt_is_causal\u001b[38;5;241m=\u001b[39mtgt_is_causal,\n\u001b[1;32m    499\u001b[0m                  memory_is_causal\u001b[38;5;241m=\u001b[39mmemory_is_causal)\n\u001b[1;32m    501\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    502\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(output)\n",
      "File \u001b[0;32m~/miniconda3/envs/smallville/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/smallville/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/smallville/lib/python3.11/site-packages/torch/nn/modules/transformer.py:890\u001b[0m, in \u001b[0;36mTransformerDecoderLayer.forward\u001b[0;34m(self, tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask, tgt_is_causal, memory_is_causal)\u001b[0m\n\u001b[1;32m    888\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ff_block(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm3(x))\n\u001b[1;32m    889\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 890\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sa_block(x, tgt_mask, tgt_key_padding_mask, tgt_is_causal))\n\u001b[1;32m    891\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mha_block(x, memory, memory_mask, memory_key_padding_mask, memory_is_causal))\n\u001b[1;32m    892\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm3(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ff_block(x))\n",
      "File \u001b[0;32m~/miniconda3/envs/smallville/lib/python3.11/site-packages/torch/nn/modules/transformer.py:899\u001b[0m, in \u001b[0;36mTransformerDecoderLayer._sa_block\u001b[0;34m(self, x, attn_mask, key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m    897\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_sa_block\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor,\n\u001b[1;32m    898\u001b[0m               attn_mask: Optional[Tensor], key_padding_mask: Optional[Tensor], is_causal: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 899\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attn(x, x, x,\n\u001b[1;32m    900\u001b[0m                        attn_mask\u001b[38;5;241m=\u001b[39mattn_mask,\n\u001b[1;32m    901\u001b[0m                        key_padding_mask\u001b[38;5;241m=\u001b[39mkey_padding_mask,\n\u001b[1;32m    902\u001b[0m                        is_causal\u001b[38;5;241m=\u001b[39mis_causal,\n\u001b[1;32m    903\u001b[0m                        need_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    904\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout1(x)\n",
      "File \u001b[0;32m~/miniconda3/envs/smallville/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/smallville/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/smallville/lib/python3.11/site-packages/torch/nn/modules/activation.py:1140\u001b[0m, in \u001b[0;36mMultiheadAttention.forward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   1136\u001b[0m     why_not_fast_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfloating-point masks are not supported for fast path.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1138\u001b[0m is_batched \u001b[38;5;241m=\u001b[39m query\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[0;32m-> 1140\u001b[0m key_padding_mask \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39m_canonical_mask(\n\u001b[1;32m   1141\u001b[0m     mask\u001b[38;5;241m=\u001b[39mkey_padding_mask,\n\u001b[1;32m   1142\u001b[0m     mask_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkey_padding_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1143\u001b[0m     other_type\u001b[38;5;241m=\u001b[39mF\u001b[38;5;241m.\u001b[39m_none_or_dtype(attn_mask),\n\u001b[1;32m   1144\u001b[0m     other_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattn_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1145\u001b[0m     target_type\u001b[38;5;241m=\u001b[39mquery\u001b[38;5;241m.\u001b[39mdtype\n\u001b[1;32m   1146\u001b[0m )\n\u001b[1;32m   1148\u001b[0m attn_mask \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39m_canonical_mask(\n\u001b[1;32m   1149\u001b[0m     mask\u001b[38;5;241m=\u001b[39mattn_mask,\n\u001b[1;32m   1150\u001b[0m     mask_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattn_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1154\u001b[0m     check_other\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   1155\u001b[0m )\n\u001b[1;32m   1157\u001b[0m is_fastpath_enabled \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mbackends\u001b[38;5;241m.\u001b[39mmha\u001b[38;5;241m.\u001b[39mget_fastpath_enabled()\n",
      "File \u001b[0;32m~/miniconda3/envs/smallville/lib/python3.11/site-packages/torch/nn/functional.py:5133\u001b[0m, in \u001b[0;36m_canonical_mask\u001b[0;34m(mask, mask_name, other_type, other_name, target_type, check_other)\u001b[0m\n\u001b[1;32m   5131\u001b[0m _mask_is_float \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mis_floating_point(mask)\n\u001b[1;32m   5132\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _mask_dtype \u001b[38;5;241m!=\u001b[39m torch\u001b[38;5;241m.\u001b[39mbool \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _mask_is_float:\n\u001b[0;32m-> 5133\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[1;32m   5134\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124monly bool and floating types of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmask_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m are supported\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   5135\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_other \u001b[38;5;129;01mand\u001b[39;00m other_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   5136\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _mask_dtype \u001b[38;5;241m!=\u001b[39m other_type:\n",
      "\u001b[0;31mAssertionError\u001b[0m: only bool and floating types of key_padding_mask are supported"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Hyperparameters\n",
    "vocab_size = len(tokenizer)\n",
    "d_model = 768  # Dimension of the model\n",
    "nhead = 12  # Number of attention heads\n",
    "num_layers = 12  # Number of transformer layers\n",
    "dim_feedforward = 3072  # Dimension of the feedforward layer\n",
    "max_seq_len = 128  # Maximum sequence length\n",
    "lr = 5e-5\n",
    "num_epochs = 3\n",
    "\n",
    "# Initialize model, optimizer, and loss function\n",
    "model = GPT2Decoder(vocab_size, d_model, nhead, num_layers, dim_feedforward, max_seq_len)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for input_ids, attention_mask, labels in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        loss = loss_fn(outputs.view(-1, vocab_size), labels.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(f\"Epoch: {epoch}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Example input sequences (padded to the same length)\n",
    "testtest = torch.tensor([\n",
    "    [101, 102, 103, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [201, 202, 203, 204, 205, 206, 207, 208, 209, 0]\n",
    "], dtype=torch.long)\n",
    "\n",
    "padding_mask = (testtest == 0).unsqueeze(1)  # (batch_size, 1, 1, seq_len)\n",
    "padding_mask = padding_mask.float().masked_fill(padding_mask == 1, float('-inf')).masked_fill(padding_mask == 0, float(0.0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 10])"
      ]
     },
     "execution_count": 414,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padding_mask.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf]],\n",
       "\n",
       "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., -inf]]])"
      ]
     },
     "execution_count": 415,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_square_subsequent_mask(sz):\n",
    "    mask = torch.triu(torch.ones(sz, sz), diagonal=1)\n",
    "    mask = mask.masked_fill(mask == 1, float('-inf')).masked_fill(mask == 0, float(0.0))\n",
    "    return mask\n",
    "\n",
    "seq_len = testtest.size(1)\n",
    "causal_mask = generate_square_subsequent_mask(seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 10])"
      ]
     },
     "execution_count": 417,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "causal_mask.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 418,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "causal_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_mask = padding_mask + causal_mask.unsqueeze(0)\n",
    "combined_mask = combined_mask.squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10, 10])"
      ]
     },
     "execution_count": 433,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_mask.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "         [0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "         [0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "         [0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "         [0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "         [0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "         [0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "         [0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "         [0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "         [0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf]],\n",
       "\n",
       "        [[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "         [0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "         [0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "         [0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "         [0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n",
       "         [0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf],\n",
       "         [0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., -inf],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., -inf]]])"
      ]
     },
     "execution_count": 434,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 64\n",
    "q = torch.rand((2, seq_len, d_model))\n",
    "k = torch.rand((2, seq_len, d_model))\n",
    "v = torch.rand((2, seq_len, d_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10, 64])"
      ]
     },
     "execution_count": 436,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_k = q.size(-1)\n",
    "scores = torch.matmul(q, k.transpose(-2, -1)) / d_k**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10, 10])"
      ]
     },
     "execution_count": 438,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10, 10])"
      ]
     },
     "execution_count": 439,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_mask.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10, 10])"
      ]
     },
     "execution_count": 440,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "         [0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "         [0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "         [0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "         [0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "         [0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "         [0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "         [0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "         [0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "         [0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf]],\n",
       "\n",
       "        [[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "         [0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "         [0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "         [0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "         [0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n",
       "         [0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf],\n",
       "         [0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., -inf],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., -inf]]])"
      ]
     },
     "execution_count": 441,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10, 10])"
      ]
     },
     "execution_count": 442,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.6500,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,\n",
       "           -inf],\n",
       "        [2.1195, 2.3881,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,\n",
       "           -inf],\n",
       "        [1.9112, 2.1823, 2.0466,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,\n",
       "           -inf],\n",
       "        [1.6999, 2.0856, 1.9427,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,\n",
       "           -inf],\n",
       "        [1.9259, 2.3343, 2.2656,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,\n",
       "           -inf],\n",
       "        [2.1642, 2.4074, 2.2990,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,\n",
       "           -inf],\n",
       "        [1.9403, 2.3798, 2.2270,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,\n",
       "           -inf],\n",
       "        [2.0873, 2.3281, 2.1804,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,\n",
       "           -inf],\n",
       "        [1.9961, 2.4710, 2.3000,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,\n",
       "           -inf],\n",
       "        [1.7350, 2.1518, 1.9802,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,\n",
       "           -inf]])"
      ]
     },
     "execution_count": 443,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores.masked_fill(combined_mask == float('-inf'), float('-inf'))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., -inf]])"
      ]
     },
     "execution_count": 459,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Example input sequences (padded to the same length)\n",
    "testtest = torch.tensor([\n",
    "    [101, 102, 103, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [201, 202, 203, 204, 205, 206, 207, 208, 209, 0]\n",
    "], dtype=torch.long)\n",
    "\n",
    "padding_mask = (testtest == 0)\n",
    "padding_mask = padding_mask.float().masked_fill(padding_mask == 1, float('-inf')).masked_fill(padding_mask == 0, float(0.0))\n",
    "padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10])"
      ]
     },
     "execution_count": 460,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padding_mask.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf]],\n",
       "\n",
       "         [[0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf]]],\n",
       "\n",
       "\n",
       "        [[[0., 0., 0., 0., 0., 0., 0., 0., 0., -inf]],\n",
       "\n",
       "         [[0., 0., 0., 0., 0., 0., 0., 0., 0., -inf]]]])"
      ]
     },
     "execution_count": 461,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padding_mask.view(2, 1, 1, 10).expand(-1, 2, -1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 462,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_square_subsequent_mask(sz):\n",
    "    mask = torch.triu(torch.ones(sz, sz), diagonal=1)\n",
    "    mask = mask.masked_fill(mask == 1, float('-inf')).masked_fill(mask == 0, float(0.0))\n",
    "    return mask\n",
    "\n",
    "seq_len = testtest.size(1)\n",
    "causal_mask = generate_square_subsequent_mask(seq_len)\n",
    "causal_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "          [0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "          [0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "          [0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "          [0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n",
       "          [0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf],\n",
       "          [0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf],\n",
       "          [0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf],\n",
       "          [0., 0., 0., 0., 0., 0., 0., 0., 0., -inf],\n",
       "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "         [[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "          [0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "          [0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "          [0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "          [0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n",
       "          [0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf],\n",
       "          [0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf],\n",
       "          [0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf],\n",
       "          [0., 0., 0., 0., 0., 0., 0., 0., 0., -inf],\n",
       "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "        [[[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "          [0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "          [0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "          [0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "          [0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n",
       "          [0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf],\n",
       "          [0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf],\n",
       "          [0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf],\n",
       "          [0., 0., 0., 0., 0., 0., 0., 0., 0., -inf],\n",
       "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "         [[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "          [0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "          [0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "          [0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "          [0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n",
       "          [0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf],\n",
       "          [0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf],\n",
       "          [0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf],\n",
       "          [0., 0., 0., 0., 0., 0., 0., 0., 0., -inf],\n",
       "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]]])"
      ]
     },
     "execution_count": 463,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "causal_mask.view(1,1,10,10).expand(2, 2, -1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "          [0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "          [0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "          [0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "          [0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "          [0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "          [0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "          [0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "          [0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "          [0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf]],\n",
       "\n",
       "         [[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "          [0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "          [0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "          [0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "          [0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "          [0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "          [0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "          [0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "          [0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "          [0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf]]],\n",
       "\n",
       "\n",
       "        [[[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "          [0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "          [0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "          [0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "          [0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n",
       "          [0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf],\n",
       "          [0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf],\n",
       "          [0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf],\n",
       "          [0., 0., 0., 0., 0., 0., 0., 0., 0., -inf],\n",
       "          [0., 0., 0., 0., 0., 0., 0., 0., 0., -inf]],\n",
       "\n",
       "         [[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "          [0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "          [0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "          [0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "          [0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n",
       "          [0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf],\n",
       "          [0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf],\n",
       "          [0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf],\n",
       "          [0., 0., 0., 0., 0., 0., 0., 0., 0., -inf],\n",
       "          [0., 0., 0., 0., 0., 0., 0., 0., 0., -inf]]]])"
      ]
     },
     "execution_count": 464,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = causal_mask.view(1,1,10,10).expand(2, 2, -1, -1)\n",
    "b = padding_mask.view(2, 1, 1, 10).expand(-1, 2, -1, -1)\n",
    "\n",
    "a + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention mask shape: torch.Size([1, 7, 7])\n",
      "Attention mask:\n",
      "tensor([[[0., -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "         [0., 0., -inf, -inf, -inf, -inf, -inf],\n",
      "         [0., 0., 0., -inf, -inf, -inf, -inf],\n",
      "         [0., 0., 0., -inf, -inf, -inf, -inf],\n",
      "         [0., 0., 0., -inf, -inf, -inf, -inf],\n",
      "         [0., 0., 0., -inf, -inf, -inf, -inf],\n",
      "         [0., 0., 0., -inf, -inf, -inf, -inf]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "# Example input sequences (batch size of 1 for simplicity, actual text length 10, padded to 30)\n",
    "input_ids = torch.tensor([\n",
    "    [101, 102, 103, 0, 0, 0, 0] # B by T\n",
    "], dtype=torch.long)\n",
    "\n",
    "# Create the padding mask\n",
    "padding_mask = (input_ids != 0).unsqueeze(1).unsqueeze(2)  # (batch_size, 1, 1, seq_len)\n",
    "padding_mask = padding_mask.float().masked_fill(padding_mask == 0, float('-inf')).masked_fill(padding_mask == 1, float(0.0))\n",
    "\n",
    "# Create the causal mask\n",
    "seq_len = input_ids.size(1)\n",
    "causal_mask = torch.triu(torch.ones((seq_len, seq_len)), diagonal=1).bool()\n",
    "causal_mask = causal_mask.float().masked_fill(causal_mask, float('-inf')).masked_fill(causal_mask == 0, float(0.0))\n",
    "\n",
    "# Combine the masks\n",
    "attention_mask = padding_mask + causal_mask.unsqueeze(0)  # (batch_size, seq_len, seq_len)\n",
    "attention_mask = attention_mask.squeeze(1)  # (batch_size, seq_len, seq_len)\n",
    "\n",
    "print(\"Attention mask shape:\", attention_mask.shape)\n",
    "print(\"Attention mask:\")\n",
    "print(attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = torch.randn(1,7,3) # B by T by Hs\n",
    "K = torch.randn(1,7,3) # B by T by Hs\n",
    "V = torch.randn(1,7,3) # B by T by Hs\n",
    "\n",
    "d_k = Q.size(-1)\n",
    "\n",
    "scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0542, 0.9458, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.2653, 0.0388, 0.6959, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1499, 0.7010, 0.1491, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.2272, 0.4889, 0.2839, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.6064, 0.1371, 0.2565, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.4870, 0.1639, 0.3491, 0.0000, 0.0000, 0.0000, 0.0000]]])"
      ]
     },
     "execution_count": 484,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei = scores.masked_fill(attention_mask == float('-inf'), float('-inf'))\n",
    "wei = torch.nn.functional.softmax(wei, dim=-1)\n",
    "wei\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0192, -0.5029,  0.0329],\n",
       "         [ 0.2423,  0.6938,  0.0231],\n",
       "         [-0.6536, -0.9839, -1.7408],\n",
       "         [ 0.0384,  0.2705, -0.3544],\n",
       "         [-0.1437, -0.1006, -0.6956],\n",
       "         [-0.1998, -0.5248, -0.6222],\n",
       "         [-0.2842, -0.5614, -0.8584]]])"
      ]
     },
     "execution_count": 488,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.matmul(wei, V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention mask shape: torch.Size([1, 10, 10])\n",
      "Attention mask:\n",
      "tensor([[[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "         [0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "         [0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "         [0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "         [0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "         [0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "         [0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "         [0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "         [0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "         [0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "# Example input sequences (batch size of 1 for simplicity, actual text length 10, padded to 30)\n",
    "input_ids = torch.tensor([\n",
    "    [101, 102, 103, 104, 0, 0, 0, 0, 0, 0]\n",
    "], dtype=torch.long)\n",
    "\n",
    "# Create the padding mask\n",
    "padding_mask = (input_ids != 0).unsqueeze(1).unsqueeze(2)  # (batch_size, 1, 1, seq_len)\n",
    "padding_mask = padding_mask.float().masked_fill(padding_mask == 0, float('-inf')).masked_fill(padding_mask == 1, float(0.0))\n",
    "\n",
    "# Create the causal mask\n",
    "seq_len = input_ids.size(1)\n",
    "causal_mask = torch.triu(torch.ones((seq_len, seq_len)), diagonal=1).bool()\n",
    "causal_mask = causal_mask.float().masked_fill(causal_mask, float('-inf')).masked_fill(causal_mask == 0, float(0.0))\n",
    "\n",
    "# Combine the masks\n",
    "attention_mask = padding_mask + causal_mask.unsqueeze(0)  # (batch_size, seq_len, seq_len)\n",
    "attention_mask = attention_mask.squeeze(1)  # (batch_size, seq_len, seq_len)\n",
    "\n",
    "print(\"Attention mask shape:\", attention_mask.shape)\n",
    "print(\"Attention mask:\")\n",
    "print(attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf]]]])"
      ]
     },
     "execution_count": 491,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention mask shape: torch.Size([1, 1, 30, 30])\n",
      "Attention mask:\n",
      "tensor([[[[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
      "           -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "          [0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
      "           -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "          [0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
      "           -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "          [0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
      "           -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "          [0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
      "           -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "          [0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
      "           -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "          [0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
      "           -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
      "           -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
      "           -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
      "           -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
      "           -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
      "           -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
      "           -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
      "           -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
      "           -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
      "           -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
      "           -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
      "           -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
      "           -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
      "           -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
      "           -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
      "           -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
      "           -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
      "           -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
      "           -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
      "           -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
      "           -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
      "           -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
      "           -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
      "           -inf, -inf, -inf, -inf, -inf, -inf, -inf]]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Example input sequences (batch size of 1 for simplicity, actual text length 10, padded to 30)\n",
    "input_ids = torch.tensor([\n",
    "    [101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "], dtype=torch.long)\n",
    "\n",
    "# Create the padding mask (1 for actual tokens, 0 for padding)\n",
    "padding_mask = (input_ids != 0).unsqueeze(1).unsqueeze(2)  # (batch_size, 1, 1, seq_len)\n",
    "\n",
    "# Create the causal mask (ensure each token can only attend to previous tokens)\n",
    "seq_len = input_ids.size(1)\n",
    "causal_mask = torch.triu(torch.ones((seq_len, seq_len)), diagonal=1)  # Upper triangular matrix with diagonal=1\n",
    "causal_mask = causal_mask == 1\n",
    "\n",
    "# Combine padding and causal masks\n",
    "combined_mask = padding_mask & (~causal_mask.unsqueeze(0))  # (batch_size, seq_len, seq_len)\n",
    "\n",
    "# Convert boolean mask to float with appropriate values for attention mechanism\n",
    "combined_mask = combined_mask.float().masked_fill(combined_mask == 0, float('-inf')).masked_fill(combined_mask == 1, float(0.0))\n",
    "\n",
    "print(\"Attention mask shape:\", combined_mask.shape)\n",
    "print(\"Attention mask:\")\n",
    "print(combined_mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention Scores after Masking:\n",
      "tensor([[[[ 0.5632,    -inf,    -inf,    -inf,    -inf],\n",
      "          [ 1.7011,  0.5329,    -inf,    -inf,    -inf],\n",
      "          [ 0.0230,  0.7564,  0.9250,    -inf,    -inf],\n",
      "          [-0.6340, -1.1890, -1.0161,  0.1602,    -inf],\n",
      "          [-1.0271, -0.4392, -2.1845,  0.4993,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[-1.9756,    -inf,    -inf,    -inf,    -inf],\n",
      "          [ 1.4239,  0.7439,    -inf,    -inf,    -inf],\n",
      "          [-0.5729,  0.3491,  0.3352,    -inf,    -inf],\n",
      "          [ 0.9252,  0.8574, -1.3857,    -inf,    -inf],\n",
      "          [ 0.7033, -0.5792, -1.4517,    -inf,    -inf]]]])\n",
      "Attention Weights after Softmax:\n",
      "tensor([[[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.7628, 0.2372, 0.0000, 0.0000, 0.0000],\n",
      "          [0.1803, 0.3754, 0.4443, 0.0000, 0.0000],\n",
      "          [0.2238, 0.1284, 0.1527, 0.4951, 0.0000],\n",
      "          [0.1296, 0.2333, 0.0407, 0.5964, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.6637, 0.3363, 0.0000, 0.0000, 0.0000],\n",
      "          [0.1668, 0.4195, 0.4137, 0.0000, 0.0000],\n",
      "          [0.4917, 0.4595, 0.0488, 0.0000, 0.0000],\n",
      "          [0.7178, 0.1991, 0.0832, 0.0000, 0.0000]]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Example input: batch of sentences with padding\n",
    "input_sequences = torch.tensor([\n",
    "    [1, 2, 3, 4, 0],  # First sentence padded with 0\n",
    "    [1, 2, 3, 0, 0]   # Second sentence padded with 0\n",
    "])\n",
    "\n",
    "# Create the padding mask (1 for padding, 0 for non-padding)\n",
    "padding_mask = (input_sequences == 0)\n",
    "\n",
    "# Create the causal mask (lower triangular matrix)\n",
    "seq_len = input_sequences.size(1)\n",
    "causal_mask = torch.tril(torch.ones(seq_len, seq_len)).unsqueeze(0).unsqueeze(0)  # Shape (1, 1, seq_len, seq_len)\n",
    "\n",
    "# Expand the padding mask (batch_size, 1, 1, seq_len)\n",
    "expanded_padding_mask = padding_mask.unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "# Combine the padding mask with the causal mask\n",
    "combined_mask = expanded_padding_mask | (causal_mask == 0)  # Logical OR to combine both masks\n",
    "\n",
    "# Example attention scores (batch_size, num_heads, seq_len, seq_len)\n",
    "attention_scores = torch.randn(2, 1, 5, 5)  # Example random scores\n",
    "\n",
    "# Apply the combined mask by adding a large negative value to the masked positions\n",
    "attention_scores = attention_scores.masked_fill(combined_mask, float('-inf'))\n",
    "\n",
    "# Compute the attention weights using softmax\n",
    "attention_weights = torch.nn.functional.softmax(attention_scores, dim=-1)\n",
    "\n",
    "print(\"Attention Scores after Masking:\")\n",
    "print(attention_scores)\n",
    "\n",
    "print(\"Attention Weights after Softmax:\")\n",
    "print(attention_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention Weights:\n",
      "tensor([[[[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "          [9.9999e-01, 1.2551e-05, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "          [2.1211e-03, 9.5765e-01, 4.0228e-02, 0.0000e+00, 0.0000e+00],\n",
      "          [2.2407e-05, 8.3682e-14, 6.6462e-12, 9.9998e-01, 0.0000e+00],\n",
      "          [2.8387e-02, 1.2374e-05, 1.9822e-05, 9.7136e-01, 2.1832e-04]],\n",
      "\n",
      "         [[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "          [9.9738e-01, 2.6188e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "          [9.0122e-03, 8.8732e-01, 1.0366e-01, 0.0000e+00, 0.0000e+00],\n",
      "          [9.4203e-03, 2.0469e-06, 9.4566e-04, 9.8963e-01, 0.0000e+00],\n",
      "          [3.0034e-01, 1.5814e-02, 5.8290e-02, 1.6572e-01, 4.5983e-01]],\n",
      "\n",
      "         [[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "          [7.5320e-03, 9.9247e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "          [8.2971e-01, 1.6469e-01, 5.6055e-03, 0.0000e+00, 0.0000e+00],\n",
      "          [4.4627e-04, 2.6121e-02, 9.5816e-01, 1.5272e-02, 0.0000e+00],\n",
      "          [7.7781e-04, 1.2386e-01, 3.5292e-01, 4.8477e-01, 3.7678e-02]]],\n",
      "\n",
      "\n",
      "        [[[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "          [2.2607e-01, 7.7393e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "          [2.0769e-01, 7.2616e-04, 7.9158e-01, 0.0000e+00, 0.0000e+00],\n",
      "          [1.4385e-01, 2.5072e-02, 2.2080e-01, 6.1028e-01, 0.0000e+00],\n",
      "          [1.6499e-01, 1.1838e-01, 1.7706e-01, 8.2168e-02, 4.5740e-01]],\n",
      "\n",
      "         [[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "          [9.3931e-01, 6.0685e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "          [8.1067e-03, 5.2007e-01, 4.7182e-01, 0.0000e+00, 0.0000e+00],\n",
      "          [4.8962e-03, 4.5854e-01, 2.0885e-01, 3.2772e-01, 0.0000e+00],\n",
      "          [2.0354e-02, 6.2437e-01, 1.5363e-01, 1.9760e-01, 4.0481e-03]],\n",
      "\n",
      "         [[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "          [9.9988e-01, 1.1988e-04, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "          [3.0678e-01, 5.0799e-01, 1.8524e-01, 0.0000e+00, 0.0000e+00],\n",
      "          [2.9244e-01, 1.2955e-01, 3.0764e-01, 2.7037e-01, 0.0000e+00],\n",
      "          [1.3339e-01, 2.9342e-01, 1.3509e-01, 1.2971e-01, 3.0838e-01]]]])\n",
      "Attention Output:\n",
      "tensor([[[ 2.9615,  0.2408,  2.6629,  0.4457,  0.3885,  4.4093],\n",
      "         [ 2.9614,  0.2408,  2.6561,  0.4451,  1.2583,  3.3029],\n",
      "         [-3.8812,  1.0462,  0.0111,  0.2759,  0.5285,  4.1860],\n",
      "         [-0.0968, -3.1557, -1.4983, -0.0788, -0.3385, -2.3922],\n",
      "         [-0.0099, -3.0596, -0.1857,  0.2418,  0.1468,  1.6252]],\n",
      "\n",
      "        [[-0.5718, -2.2407, -2.3742, -0.0627,  0.3650, -0.9575],\n",
      "         [ 0.3937, -4.3949, -2.1308, -0.0770,  0.3651, -0.9573],\n",
      "         [ 0.8625, -1.9300, -0.6167,  0.4950,  0.5573, -0.3508],\n",
      "         [ 0.9755, -2.2770, -0.1750,  0.3678, -0.3142, -0.2531],\n",
      "         [ 0.0320,  0.0244,  0.3458,  0.1573,  0.2621,  0.5403]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Parameters\n",
    "batch_size = 2\n",
    "seq_len = 5\n",
    "embed_dim = 6  # Embedding dimension (must be divisible by num_heads)\n",
    "num_heads = 3\n",
    "head_dim = embed_dim // num_heads\n",
    "\n",
    "# Generate random input sequences (batch_size, seq_len, embed_dim)\n",
    "input_sequences = torch.randn(batch_size, seq_len, embed_dim)\n",
    "\n",
    "# Create query, key, and value matrices using random weights\n",
    "W_q = torch.randn(embed_dim, embed_dim)\n",
    "W_k = torch.randn(embed_dim, embed_dim)\n",
    "W_v = torch.randn(embed_dim, embed_dim)\n",
    "\n",
    "# Compute Q, K, V matrices (batch_size, seq_len, embed_dim)\n",
    "Q = torch.matmul(input_sequences, W_q)\n",
    "K = torch.matmul(input_sequences, W_k)\n",
    "V = torch.matmul(input_sequences, W_v)\n",
    "\n",
    "# Reshape Q, K, V for multi-head attention (batch_size, num_heads, seq_len, head_dim)\n",
    "Q = Q.view(batch_size, seq_len, num_heads, head_dim).transpose(1, 2)\n",
    "K = K.view(batch_size, seq_len, num_heads, head_dim).transpose(1, 2)\n",
    "V = V.view(batch_size, seq_len, num_heads, head_dim).transpose(1, 2)\n",
    "\n",
    "# Compute scaled dot-product attention scores (batch_size, num_heads, seq_len, seq_len)\n",
    "scores = torch.matmul(Q, K.transpose(-2, -1)) / (head_dim ** 0.5)\n",
    "\n",
    "# Create causal mask (lower triangular matrix)\n",
    "causal_mask = torch.tril(torch.ones(seq_len, seq_len)).unsqueeze(0).unsqueeze(0).expand(batch_size, num_heads, -1, -1)\n",
    "\n",
    "# Apply the causal mask to the scores\n",
    "scores = scores.masked_fill(causal_mask == 0, -1e9)\n",
    "\n",
    "# Compute the attention weights using softmax (batch_size, num_heads, seq_len, seq_len)\n",
    "attention_weights = F.softmax(scores, dim=-1)\n",
    "\n",
    "# Compute the attention output (batch_size, num_heads, seq_len, head_dim)\n",
    "attention_output = torch.matmul(attention_weights, V)\n",
    "\n",
    "# Reshape the attention output back to (batch_size, seq_len, embed_dim)\n",
    "attention_output = attention_output.transpose(1, 2).contiguous().view(batch_size, seq_len, embed_dim)\n",
    "\n",
    "print(\"Attention Weights:\")\n",
    "print(attention_weights)\n",
    "\n",
    "print(\"Attention Output:\")\n",
    "print(attention_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 5, 6])"
      ]
     },
     "execution_count": 501,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 2.9615,  0.2408,  2.6629,  0.4457,  0.3885,  4.4093],\n",
       "         [ 2.9614,  0.2408,  2.6561,  0.4451,  1.2583,  3.3029],\n",
       "         [-3.8812,  1.0462,  0.0111,  0.2759,  0.5285,  4.1860],\n",
       "         [-0.0968, -3.1557, -1.4983, -0.0788, -0.3385, -2.3922],\n",
       "         [-0.0099, -3.0596, -0.1857,  0.2418,  0.1468,  1.6252]],\n",
       "\n",
       "        [[-0.5718, -2.2407, -2.3742, -0.0627,  0.3650, -0.9575],\n",
       "         [ 0.3937, -4.3949, -2.1308, -0.0770,  0.3651, -0.9573],\n",
       "         [ 0.8625, -1.9300, -0.6167,  0.4950,  0.5573, -0.3508],\n",
       "         [ 0.9755, -2.2770, -0.1750,  0.3678, -0.3142, -0.2531],\n",
       "         [ 0.0320,  0.0244,  0.3458,  0.1573,  0.2621,  0.5403]]])"
      ]
     },
     "execution_count": 499,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 5, 5])"
      ]
     },
     "execution_count": 502,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [9.9999e-01, 1.2551e-05, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [2.1211e-03, 9.5765e-01, 4.0228e-02, 0.0000e+00, 0.0000e+00],\n",
       "          [2.2407e-05, 8.3682e-14, 6.6462e-12, 9.9998e-01, 0.0000e+00],\n",
       "          [2.8387e-02, 1.2374e-05, 1.9822e-05, 9.7136e-01, 2.1832e-04]],\n",
       "\n",
       "         [[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [9.9738e-01, 2.6188e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [9.0122e-03, 8.8732e-01, 1.0366e-01, 0.0000e+00, 0.0000e+00],\n",
       "          [9.4203e-03, 2.0469e-06, 9.4566e-04, 9.8963e-01, 0.0000e+00],\n",
       "          [3.0034e-01, 1.5814e-02, 5.8290e-02, 1.6572e-01, 4.5983e-01]],\n",
       "\n",
       "         [[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [7.5320e-03, 9.9247e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [8.2971e-01, 1.6469e-01, 5.6055e-03, 0.0000e+00, 0.0000e+00],\n",
       "          [4.4627e-04, 2.6121e-02, 9.5816e-01, 1.5272e-02, 0.0000e+00],\n",
       "          [7.7781e-04, 1.2386e-01, 3.5292e-01, 4.8477e-01, 3.7678e-02]]],\n",
       "\n",
       "\n",
       "        [[[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [2.2607e-01, 7.7393e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [2.0769e-01, 7.2616e-04, 7.9158e-01, 0.0000e+00, 0.0000e+00],\n",
       "          [1.4385e-01, 2.5072e-02, 2.2080e-01, 6.1028e-01, 0.0000e+00],\n",
       "          [1.6499e-01, 1.1838e-01, 1.7706e-01, 8.2168e-02, 4.5740e-01]],\n",
       "\n",
       "         [[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [9.3931e-01, 6.0685e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [8.1067e-03, 5.2007e-01, 4.7182e-01, 0.0000e+00, 0.0000e+00],\n",
       "          [4.8962e-03, 4.5854e-01, 2.0885e-01, 3.2772e-01, 0.0000e+00],\n",
       "          [2.0354e-02, 6.2437e-01, 1.5363e-01, 1.9760e-01, 4.0481e-03]],\n",
       "\n",
       "         [[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [9.9988e-01, 1.1988e-04, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [3.0678e-01, 5.0799e-01, 1.8524e-01, 0.0000e+00, 0.0000e+00],\n",
       "          [2.9244e-01, 1.2955e-01, 3.0764e-01, 2.7037e-01, 0.0000e+00],\n",
       "          [1.3339e-01, 2.9342e-01, 1.3509e-01, 1.2971e-01, 3.0838e-01]]]])"
      ]
     },
     "execution_count": 500,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention Weights:\n",
      "tensor([[[[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "          [8.6617e-01, 1.3383e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "          [9.3948e-01, 5.7055e-02, 3.4649e-03, 0.0000e+00, 0.0000e+00],\n",
      "          [9.7613e-01, 2.3301e-02, 5.5622e-04, 1.3278e-05, 0.0000e+00],\n",
      "          [2.5000e-01, 2.5000e-01, 2.5000e-01, 2.5000e-01, 0.0000e+00]],\n",
      "\n",
      "         [[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "          [7.1849e-01, 2.8151e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "          [7.6606e-01, 1.8787e-01, 4.6074e-02, 0.0000e+00, 0.0000e+00],\n",
      "          [8.4696e-01, 1.3002e-01, 1.9959e-02, 3.0638e-03, 0.0000e+00],\n",
      "          [2.5000e-01, 2.5000e-01, 2.5000e-01, 2.5000e-01, 0.0000e+00]],\n",
      "\n",
      "         [[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "          [9.8688e-01, 1.3124e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "          [9.9847e-01, 1.5313e-03, 2.3484e-06, 0.0000e+00, 0.0000e+00],\n",
      "          [9.9982e-01, 1.7683e-04, 3.1273e-08, 5.5308e-12, 0.0000e+00],\n",
      "          [2.5000e-01, 2.5000e-01, 2.5000e-01, 2.5000e-01, 0.0000e+00]]],\n",
      "\n",
      "\n",
      "        [[[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "          [8.6617e-01, 1.3383e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "          [9.3948e-01, 5.7055e-02, 3.4649e-03, 0.0000e+00, 0.0000e+00],\n",
      "          [3.3333e-01, 3.3333e-01, 3.3333e-01, 0.0000e+00, 0.0000e+00],\n",
      "          [3.3333e-01, 3.3333e-01, 3.3333e-01, 0.0000e+00, 0.0000e+00]],\n",
      "\n",
      "         [[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "          [7.1849e-01, 2.8151e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "          [7.6606e-01, 1.8787e-01, 4.6074e-02, 0.0000e+00, 0.0000e+00],\n",
      "          [3.3333e-01, 3.3333e-01, 3.3333e-01, 0.0000e+00, 0.0000e+00],\n",
      "          [3.3333e-01, 3.3333e-01, 3.3333e-01, 0.0000e+00, 0.0000e+00]],\n",
      "\n",
      "         [[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "          [9.8688e-01, 1.3124e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "          [9.9847e-01, 1.5313e-03, 2.3484e-06, 0.0000e+00, 0.0000e+00],\n",
      "          [3.3333e-01, 3.3333e-01, 3.3333e-01, 0.0000e+00, 0.0000e+00],\n",
      "          [3.3333e-01, 3.3333e-01, 3.3333e-01, 0.0000e+00, 0.0000e+00]]]])\n",
      "Attention Output:\n",
      "tensor([[[ 3.2964,  0.9495,  1.0972,  2.3156,  0.3508, -1.7565],\n",
      "         [ 3.7375,  1.0766,  1.4060,  2.9675,  0.3554, -1.7796],\n",
      "         [ 3.5073,  1.0102,  1.4044,  2.9640,  0.3514, -1.7592],\n",
      "         [ 3.3770,  0.9727,  1.2937,  2.7304,  0.3509, -1.7568],\n",
      "         [ 8.2410,  2.3737,  2.7429,  5.7890,  0.8770, -4.3913]],\n",
      "\n",
      "        [[ 3.2964,  0.9495,  1.0972,  2.3156,  0.3508, -1.7565],\n",
      "         [ 3.7375,  1.0766,  1.4060,  2.9675,  0.3554, -1.7796],\n",
      "         [ 3.5073,  1.0102,  1.4044,  2.9640,  0.3514, -1.7592],\n",
      "         [ 6.5928,  1.8990,  2.1943,  4.6312,  0.7016, -3.5130],\n",
      "         [ 6.5928,  1.8990,  2.1943,  4.6312,  0.7016, -3.5130]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Parameters\n",
    "batch_size = 2\n",
    "seq_len = 5\n",
    "embed_dim = 6  # Embedding dimension (must be divisible by num_heads)\n",
    "num_heads = 3\n",
    "head_dim = embed_dim // num_heads\n",
    "\n",
    "# Generate input sequences with padding (0 represents padding)\n",
    "input_sequences = torch.tensor([\n",
    "    [1, 2, 3, 4, 0],  # First sentence with padding\n",
    "    [1, 2, 3, 0, 0]   # Second sentence with more padding\n",
    "], dtype=torch.float).unsqueeze(-1).repeat(1, 1, embed_dim)\n",
    "\n",
    "# Create a padding mask (1 for padding, 0 for non-padding)\n",
    "padding_mask = (input_sequences[:, :, 0] == 0)\n",
    "\n",
    "# Create query, key, and value matrices using random weights\n",
    "W_q = torch.randn(embed_dim, embed_dim)\n",
    "W_k = torch.randn(embed_dim, embed_dim)\n",
    "W_v = torch.randn(embed_dim, embed_dim)\n",
    "\n",
    "# Compute Q, K, V matrices (batch_size, seq_len, embed_dim)\n",
    "Q = torch.matmul(input_sequences, W_q)\n",
    "K = torch.matmul(input_sequences, W_k)\n",
    "V = torch.matmul(input_sequences, W_v)\n",
    "\n",
    "# Reshape Q, K, V for multi-head attention (batch_size, num_heads, seq_len, head_dim)\n",
    "Q = Q.view(batch_size, seq_len, num_heads, head_dim).transpose(1, 2)\n",
    "K = K.view(batch_size, seq_len, num_heads, head_dim).transpose(1, 2)\n",
    "V = V.view(batch_size, seq_len, num_heads, head_dim).transpose(1, 2)\n",
    "\n",
    "# Compute scaled dot-product attention scores (batch_size, num_heads, seq_len, seq_len)\n",
    "scores = torch.matmul(Q, K.transpose(-2, -1)) / (head_dim ** 0.5)\n",
    "\n",
    "# Create causal mask (lower triangular matrix)\n",
    "causal_mask = torch.tril(torch.ones(seq_len, seq_len)).unsqueeze(0).unsqueeze(0).expand(batch_size, num_heads, -1, -1)\n",
    "\n",
    "# Expand padding mask (batch_size, 1, 1, seq_len) and combine with causal mask\n",
    "expanded_padding_mask = padding_mask.unsqueeze(1).unsqueeze(2).expand(batch_size, num_heads, seq_len, seq_len)\n",
    "combined_mask = expanded_padding_mask | (causal_mask == 0)  # Logical OR to combine both masks\n",
    "\n",
    "# Apply the combined mask to the scores\n",
    "scores = scores.masked_fill(combined_mask, -1e9)\n",
    "\n",
    "# Compute the attention weights using softmax (batch_size, num_heads, seq_len, seq_len)\n",
    "attention_weights = F.softmax(scores, dim=-1)\n",
    "\n",
    "# Compute the attention output (batch_size, num_heads, seq_len, head_dim)\n",
    "attention_output = torch.matmul(attention_weights, V)\n",
    "\n",
    "# Reshape the attention output back to (batch_size, seq_len, embed_dim)\n",
    "attention_output = attention_output.transpose(1, 2).contiguous().view(batch_size, seq_len, embed_dim)\n",
    "\n",
    "print(\"Attention Weights:\")\n",
    "print(attention_weights)\n",
    "\n",
    "print(\"Attention Output:\")\n",
    "print(attention_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 5, 5])"
      ]
     },
     "execution_count": 504,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [8.6617e-01, 1.3383e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [9.3948e-01, 5.7055e-02, 3.4649e-03, 0.0000e+00, 0.0000e+00],\n",
       "          [9.7613e-01, 2.3301e-02, 5.5622e-04, 1.3278e-05, 0.0000e+00],\n",
       "          [2.5000e-01, 2.5000e-01, 2.5000e-01, 2.5000e-01, 0.0000e+00]],\n",
       "\n",
       "         [[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [7.1849e-01, 2.8151e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [7.6606e-01, 1.8787e-01, 4.6074e-02, 0.0000e+00, 0.0000e+00],\n",
       "          [8.4696e-01, 1.3002e-01, 1.9959e-02, 3.0638e-03, 0.0000e+00],\n",
       "          [2.5000e-01, 2.5000e-01, 2.5000e-01, 2.5000e-01, 0.0000e+00]],\n",
       "\n",
       "         [[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [9.8688e-01, 1.3124e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [9.9847e-01, 1.5313e-03, 2.3484e-06, 0.0000e+00, 0.0000e+00],\n",
       "          [9.9982e-01, 1.7683e-04, 3.1273e-08, 5.5308e-12, 0.0000e+00],\n",
       "          [2.5000e-01, 2.5000e-01, 2.5000e-01, 2.5000e-01, 0.0000e+00]]],\n",
       "\n",
       "\n",
       "        [[[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [8.6617e-01, 1.3383e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [9.3948e-01, 5.7055e-02, 3.4649e-03, 0.0000e+00, 0.0000e+00],\n",
       "          [3.3333e-01, 3.3333e-01, 3.3333e-01, 0.0000e+00, 0.0000e+00],\n",
       "          [3.3333e-01, 3.3333e-01, 3.3333e-01, 0.0000e+00, 0.0000e+00]],\n",
       "\n",
       "         [[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [7.1849e-01, 2.8151e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [7.6606e-01, 1.8787e-01, 4.6074e-02, 0.0000e+00, 0.0000e+00],\n",
       "          [3.3333e-01, 3.3333e-01, 3.3333e-01, 0.0000e+00, 0.0000e+00],\n",
       "          [3.3333e-01, 3.3333e-01, 3.3333e-01, 0.0000e+00, 0.0000e+00]],\n",
       "\n",
       "         [[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [9.8688e-01, 1.3124e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [9.9847e-01, 1.5313e-03, 2.3484e-06, 0.0000e+00, 0.0000e+00],\n",
       "          [3.3333e-01, 3.3333e-01, 3.3333e-01, 0.0000e+00, 0.0000e+00],\n",
       "          [3.3333e-01, 3.3333e-01, 3.3333e-01, 0.0000e+00, 0.0000e+00]]]])"
      ]
     },
     "execution_count": 505,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 5, 6])"
      ]
     },
     "execution_count": 506,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 3.2964,  0.9495,  1.0972,  2.3156,  0.3508, -1.7565],\n",
       "         [ 3.7375,  1.0766,  1.4060,  2.9675,  0.3554, -1.7796],\n",
       "         [ 3.5073,  1.0102,  1.4044,  2.9640,  0.3514, -1.7592],\n",
       "         [ 3.3770,  0.9727,  1.2937,  2.7304,  0.3509, -1.7568],\n",
       "         [ 8.2410,  2.3737,  2.7429,  5.7890,  0.8770, -4.3913]],\n",
       "\n",
       "        [[ 3.2964,  0.9495,  1.0972,  2.3156,  0.3508, -1.7565],\n",
       "         [ 3.7375,  1.0766,  1.4060,  2.9675,  0.3554, -1.7796],\n",
       "         [ 3.5073,  1.0102,  1.4044,  2.9640,  0.3514, -1.7592],\n",
       "         [ 6.5928,  1.8990,  2.1943,  4.6312,  0.7016, -3.5130],\n",
       "         [ 6.5928,  1.8990,  2.1943,  4.6312,  0.7016, -3.5130]]])"
      ]
     },
     "execution_count": 507,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Example usage\n",
    "input_dim = 5\n",
    "seq_len = 5\n",
    "num_heads = 8\n",
    "batch_size = 2\n",
    "\n",
    "# Create example input tensor (batch_size, seq_len, input_dim)\n",
    "input_tensor = torch.tensor([\n",
    "    [1, 2, 3, 4, 0],  # First sentence with padding\n",
    "    [1, 2, 3, 0, 0]   # Second sentence with more padding\n",
    "])\n",
    "\n",
    "# Create causal mask tensor (batch_size, seq_len, seq_len)\n",
    "causal_mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).unsqueeze(0)  # Upper triangular matrix\n",
    "causal_mask = causal_mask == 1\n",
    "\n",
    "# Create padding mask tensor (batch_size, seq_len)\n",
    "padding_mask = (input_tensor == 0).unsqueeze(1)  # Padding tokens are zeros\n",
    "\n",
    "# Combine the masks (padding_mask will have zeros for padded tokens)\n",
    "mask = causal_mask + padding_mask.unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3, 4, 0],\n",
       "        [1, 2, 3, 0, 0]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[False,  True,  True,  True,  True],\n",
       "         [False, False,  True,  True,  True],\n",
       "         [False, False, False,  True,  True],\n",
       "         [False, False, False, False,  True],\n",
       "         [False, False, False, False, False]]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "causal_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[False, False, False, False,  True]],\n",
       "\n",
       "        [[False, False, False,  True,  True]]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[False,  True,  True,  True,  True],\n",
       "          [False, False,  True,  True,  True],\n",
       "          [False, False, False,  True,  True],\n",
       "          [False, False, False, False,  True],\n",
       "          [ True,  True,  True,  True,  True]]],\n",
       "\n",
       "\n",
       "        [[[False,  True,  True,  True,  True],\n",
       "          [False, False,  True,  True,  True],\n",
       "          [False, False, False,  True,  True],\n",
       "          [ True,  True,  True,  True,  True],\n",
       "          [ True,  True,  True,  True,  True]]]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1, 0, 0, 0, 0],\n",
       "         [1, 1, 0, 0, 0],\n",
       "         [1, 1, 1, 0, 0],\n",
       "         [1, 1, 1, 1, 0],\n",
       "         [0, 0, 0, 0, 0]],\n",
       "\n",
       "        [[1, 0, 0, 0, 0],\n",
       "         [1, 1, 0, 0, 0],\n",
       "         [1, 1, 1, 0, 0],\n",
       "         [0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0]]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.where(mask.squeeze(1), torch.tensor(0), torch.tensor(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[1., 0., 0., 0., 0.],\n",
       "          [1., 1., 0., 0., 0.],\n",
       "          [1., 1., 1., 0., 0.],\n",
       "          [1., 1., 1., 1., 0.],\n",
       "          [1., 1., 1., 1., 1.]]]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tril(torch.ones(seq_len, seq_len)).unsqueeze(0).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
