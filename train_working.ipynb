{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.functional import F\n",
    "\n",
    "## ----- Hyperparameters ------\n",
    "cfg = {\n",
    "    'batch_size': 64, # B\n",
    "    'block_size': 128, # T\n",
    "    'n_embed': 64, # C\n",
    "    'eval_iters': 10, # number of iterations to evaluate the model at a time step\n",
    "    'max_iters': 3000, # number of iterations to train the model\n",
    "    'eter_interval': 100, # interval to evaluate the model\n",
    "    'learning_rate': 1e-3,\n",
    "    'head_size': 8, # dimension of the head\n",
    "    'num_heads': 8, # number of heads\n",
    "    'num_layers': 4, # number of layers\n",
    "    'dropout': 0.1 # dropout \n",
    "}\n",
    "\n",
    "batch_size = cfg['batch_size']\n",
    "block_size = cfg['block_size']\n",
    "n_embed = cfg['n_embed']\n",
    "eval_iters = cfg['eval_iters']\n",
    "max_iters = cfg['max_iters']\n",
    "eter_interval = cfg['eter_interval']\n",
    "learning_rate = cfg['learning_rate']\n",
    "head_size = cfg['head_size']\n",
    "num_heads = cfg['num_heads']\n",
    "num_layers = cfg['num_layers']\n",
    "dropout = cfg['dropout']\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trainers.dataloader import DataLoader\n",
    "\n",
    "dataloader = DataLoader('input.txt', block_size, batch_size, vocab_size = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train','val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for i in range(eval_iters):\n",
    "            x,y = dataloader.load(split)\n",
    "            logits, loss = model(x, y)\n",
    "            losses[i] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 7. Create the attention mechanism\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.head_size = head_size\n",
    "        self.q = nn.Linear(n_embed, head_size)\n",
    "        self.k = nn.Linear(n_embed, head_size)\n",
    "        self.v = nn.Linear(n_embed, head_size)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size))) # used to mask the attention matrix\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        Q = self.q(x)\n",
    "        K = self.k(x)\n",
    "        V = self.v(x)\n",
    "\n",
    "        ## Scaled dot product attention\n",
    "        attention = Q @ K.transpose(-2,-1) * (C ** -0.5) # B,T,T\n",
    "        attention = attention.masked_fill(self.tril[:T,:T] == 0, float('-inf')) # B,T,T\n",
    "        attention = F.softmax(attention, dim=-1) # B,T,T\n",
    "        attention = self.dropout(attention)\n",
    "        out = attention @ V # B,T,H\n",
    "        return out\n",
    "    \n",
    "## 8. Create the multi-head attention mechanism\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, head_size, num_heads):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.linear = nn.Linear(head_size*num_heads, n_embed) # projection layer going back into the pathway\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.linear(out))\n",
    "        return out\n",
    "    \n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, n_embed):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embed,4*n_embed),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4*n_embed,n_embed), # projection layer going back into the residual pathway\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    \n",
    "class Block(nn.Module):\n",
    "    def __init__(self, n_embed, num_heads):\n",
    "        super().__init__()\n",
    "        head_size = n_embed // num_heads\n",
    "        self.sa_head = MultiHeadAttention(head_size, num_heads)\n",
    "        self.ffn = FeedForward(n_embed)\n",
    "        self.ln1 = nn.LayerNorm(n_embed)\n",
    "        self.ln2 = nn.LayerNorm(n_embed)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa_head(self.ln1(x)) # pre-normalisation\n",
    "        x = x + self.ffn(self.ln2(x)) # pre-normalisation\n",
    "        return x\n",
    "\n",
    "## 7. Define the model\n",
    "class SuperSimpleBigramModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.embedding_table = nn.Embedding(vocab_size, n_embed)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embed)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embed, num_heads) for _ in range(num_layers)])\n",
    "        self.ln_final = nn.LayerNorm(n_embed)\n",
    "        self.lm_head = nn.Linear(n_embed, vocab_size)\n",
    "        \n",
    "    def forward(self, idx, targets = None):\n",
    "        B,T = idx.shape\n",
    "        tok_emb = self.embedding_table(idx)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T).to(device))\n",
    "        x = tok_emb + pos_emb\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_final(x)\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "\n",
    "        if targets == None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B,T,C = logits.shape\n",
    "            logits = logits.view(B*T,C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_trun = idx[:,-block_size:] # truncate the context\n",
    "            logits, _ = self(idx_trun)\n",
    "            logits = logits[:,-1,:] \n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx\n",
    "\n",
    "\n",
    "with open('input.txt','r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "from tokenizer.characters import characters\n",
    "\n",
    "characters = characters(text)\n",
    "\n",
    "## 8. Initialize the model\n",
    "model = SuperSimpleBigramModel(characters.vocab_size).to(device)\n",
    "\n",
    "## 9. Create an optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.gpt2 import GPT\n",
    "\n",
    "model = GPT(characters.vocab_size, num_layers, num_heads, n_embed, block_size, dropout).to(device)\n",
    "\n",
    "## 9. Create an optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0, Train loss: 4.31210470199585, Val loss: 4.32597541809082\n",
      "Iter 100, Train loss: 2.605957508087158, Val loss: 2.6148741245269775\n",
      "Iter 200, Train loss: 2.4959640502929688, Val loss: 2.5022177696228027\n",
      "Iter 300, Train loss: 2.439072847366333, Val loss: 2.440169095993042\n",
      "Iter 400, Train loss: 2.3691036701202393, Val loss: 2.378167152404785\n",
      "Iter 500, Train loss: 2.2947041988372803, Val loss: 2.3070859909057617\n",
      "Iter 600, Train loss: 2.215425968170166, Val loss: 2.2448205947875977\n",
      "Iter 700, Train loss: 2.167931318283081, Val loss: 2.2017030715942383\n",
      "Iter 800, Train loss: 2.11657977104187, Val loss: 2.1560561656951904\n",
      "Iter 900, Train loss: 2.055384874343872, Val loss: 2.1246914863586426\n",
      "Iter 1000, Train loss: 2.0358901023864746, Val loss: 2.088453531265259\n",
      "Iter 1100, Train loss: 1.9923267364501953, Val loss: 2.0723185539245605\n",
      "Iter 1200, Train loss: 1.9553769826889038, Val loss: 2.0297110080718994\n",
      "Iter 1300, Train loss: 1.923105001449585, Val loss: 2.0168614387512207\n",
      "Iter 1400, Train loss: 1.896121621131897, Val loss: 1.9920730590820312\n",
      "Iter 1500, Train loss: 1.8598802089691162, Val loss: 1.9757487773895264\n",
      "Iter 1600, Train loss: 1.8481556177139282, Val loss: 1.9543039798736572\n",
      "Iter 1700, Train loss: 1.8298925161361694, Val loss: 1.9419139623641968\n",
      "Iter 1800, Train loss: 1.8010051250457764, Val loss: 1.9347248077392578\n",
      "Iter 1900, Train loss: 1.7906196117401123, Val loss: 1.920751929283142\n",
      "Iter 2000, Train loss: 1.7837703227996826, Val loss: 1.9073972702026367\n",
      "Iter 2100, Train loss: 1.7502076625823975, Val loss: 1.895552396774292\n",
      "Iter 2200, Train loss: 1.745775580406189, Val loss: 1.8859087228775024\n",
      "Iter 2300, Train loss: 1.7277288436889648, Val loss: 1.8809521198272705\n",
      "Iter 2400, Train loss: 1.7286850214004517, Val loss: 1.86603581905365\n",
      "Iter 2500, Train loss: 1.7159700393676758, Val loss: 1.8663009405136108\n",
      "Iter 2600, Train loss: 1.6941258907318115, Val loss: 1.8475637435913086\n",
      "Iter 2700, Train loss: 1.6874206066131592, Val loss: 1.8565384149551392\n",
      "Iter 2800, Train loss: 1.6805709600448608, Val loss: 1.8401644229888916\n",
      "Iter 2900, Train loss: 1.6619819402694702, Val loss: 1.8175817728042603\n"
     ]
    }
   ],
   "source": [
    "## 10. Train the model\n",
    "for iter in range(max_iters):\n",
    "    \n",
    "    if iter % eter_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f'Iter {iter}, Train loss: {losses[\"train\"]}, Val loss: {losses[\"val\"]}')\n",
    "        # if wandb_log:\n",
    "        #     wandb.log({'train_loss': losses['train'], 'val_loss': losses['val']})\n",
    "    \n",
    "    xb, yb = dataloader.load('train')\n",
    "\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "smallville",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
