### Introduction

Lately, I have found the developments of LLMs to be highly fasciinating. This is mainly because I'm working mainly on LLMs in my day job as a LLM Research Engineer. Inspired by the great minds around me, I hope to understand the workings of LLMs from under the hood. The goal of this repository is meant to be a learning project myself. I'm sure there are many who have already built their own and made them quite robust. I hope to catch up.

Where possible, I will share all my resources that have helped me understood more about LLMs. Simultaneously, I hope to append my knowledge on why certain things have helped. 

### References

Karparthy, A. (2024). Letâ€™s build the GPT tokenizer. Retrieved from https://youtu.be/zduSFxRajkE?si=QIVyM_tgpHQ3T5RH 

Karparthy, A. (2023). Let's build GPT: from scratch, in code, spelled out. Retrieved from https://youtu.be/kCc8FmEb1nY?si=cyIGm83Kb26eBsmD

Warner, B. (2023). Creating a Transformer From Scratch - Part One: The Attention Mechanism. Retrieved from https://benjaminwarner.dev/2023/07/01/attention-mechanism