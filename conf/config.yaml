defaults:
  - languagemodel: ??? # language model name
  - model: 'gpt2' # model name
  - tokenizer: 'characternew' # tokenizer name
  - _self_ # self reference

# input:
train_file        : 'data/input.txt'  # training data
train_size        : 0.9     # training data size
batch_size        : 64      # B
block_size        : 128     # T
n_embed           : 64      # C
vocab_size        : ???     # V # assigned by tokenizer upon loading dataloader object
use_rope          : True    # Rotational Positional Encoding (RoPE)

# attn:
num_heads         : 8       # H
num_layers        : 4       # L

# eval:
eval_iters        : 10      # number of iterations to evaluate
eval_interval     : 100     # interval to evaluate

# train:
max_iters         : 3000    # number of iterations to train
lr_scheduler:
  name            : 'cosine' # learning rate scheduler type
  initial_lr      : 0.001   # initial learning rate
  min_lr          : 0.00001  # final learning rate
  total_steps     : ${max_iters}    # total steps to decay learning rate
  warmup_steps    : 100     # warmup steps
dropout_scheduler:
  name            : 'constant' # dropout rate scheduler type
  initial_dropout : 0.1     # initial dropout rate
  final_dropout   : 0.3     # final dropout rate
  total_steps     : ${max_iters}    # total steps to decay dropout rate

# registry:
model_ckpt_dir    : 'model/checkpoints' # model checkpoint directory        
model_ckpt        : '${languagemodel.name}_${tokenizer.name}_model_checkpoint.pth' # model checkpoint
# optimizer         : 'adam'  # optimizer name 
# scheduler         : 'linear'

# wandb:
wandb_log         : False   # use wandb
wandb_project     : 'smallville'  # wandb project name
