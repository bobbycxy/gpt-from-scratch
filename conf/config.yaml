defaults:
  - languagemodel: ??? # language model name
  - model: 'gpt2' # model name
  - tokenizer: 'characternew' # tokenizer name
  - scheduler: 'default' # scheduler name
  - _self_ # self reference

# input:
train_file        : 'data/input.txt'  # training data
train_size        : 0.9     # training data size
batch_size        : 64      # B
block_size        : 128     # T
n_embed           : 64      # C
vocab_size        : ???     # V # assigned by tokenizer upon loading dataloader object
use_rope          : True    # Rotational Positional Encoding (RoPE)

# attn:
num_heads         : 8       # H
num_layers        : 4       # L

# eval:
eval_iters        : 10      # number of iterations to evaluate
eval_interval     : 100     # interval to evaluate

# train:
max_iters         : 3000    # number of iterations to train

# registry:
model_ckpt_dir    : 'model/checkpoints' # model checkpoint directory        
model_ckpt        : '${languagemodel.name}_${tokenizer.name}_model_checkpoint.pth' # model checkpoint
# optimizer         : 'adam'  # optimizer name 
# scheduler         : 'linear'

# wandb:
wandb_log         : False   # use wandb
wandb_project     : 'smallville'  # wandb project name
