defaults:
  - model: 'gpt2'
  - tokenizer: 'character'
  - _self_

# input:
train_file        : 'data/input.txt'  # training data
batch_size        : 64      # B
block_size        : 128     # T
n_embed           : 64      # C
vocab_size        : ???      # V

# attn:
num_heads         : 8       # H
num_layers        : 4       # L

# eval:
eval_iters        : 10      # number of iterations to evaluate
eval_interval     : 100     # interval to evaluate

# train:
max_iters         : 3000    # number of iterations to train
learning_rate     : 0.001  # learning rate
dropout           : 0.1     # dropout rate

# registry:
optimizer         : 'adam'  # optimizer name
train_size        : 0.9     # training data size
# scheduler         : 'linear'

# masked_lm:
masked_lm         : False    # whether to use masked language model
is_causal         : True    # whether to use causal mask
mlm_prob          : 0.15    # mask probability